# üöÄ Sophia-Artemis Cloud Deployment Optimization Strategy

## Executive Summary

This document presents a comprehensive optimization strategy for the Sophia-Artemis hybrid intelligence platform's cloud deployment, analyzing current Fly.io infrastructure and proposing improvements for multi-region scaling, GPU integration, and cost optimization.

---

## üìä Current Infrastructure Analysis

### Fly.io Deployment Status

| Service | Region | Resources | Auto-scaling | Status |
|---------|--------|-----------|--------------|---------|
| sophia-api | sjc | 4 CPU, 4GB RAM | 2-20 machines | ‚úÖ Configured |
| sophia-bridge | sjc | 2 CPU, 2GB RAM | 1-10 machines | ‚úÖ Configured |
| sophia-mcp | sjc | 2 CPU, 2GB RAM | 1-10 machines | ‚úÖ Configured |
| sophia-vector | sjc | 2 CPU, 2GB RAM | 1-10 machines | ‚úÖ Configured |
| sophia-ui | sjc | 1 CPU, 1GB RAM | 1-5 machines | ‚úÖ Configured |

### Infrastructure Gaps

1. **Single Region Deployment**: All services in San Jose (sjc)
2. **No GPU Integration**: Lambda Labs endpoint configured but not utilized
3. **Basic Deployment Strategy**: No blue-green or canary deployments
4. **Limited Monitoring**: Basic health checks without distributed tracing
5. **Manual Scaling Policies**: Auto-scaling based on simple CPU/memory metrics

---

## üéØ Optimization Strategy

### 1. Optimized Single-User Deployment Architecture (Las Vegas)

```yaml
regions:
  primary: sjc  # San Jose - 370 miles from Las Vegas (~15ms latency)
  backup: lax  # Los Angeles - 270 miles from Las Vegas (~10ms latency)
  
deployment_strategy:
  type: single-tenant-optimized
  user_location: las_vegas_nv
  optimization_focus: latency_and_cost
  redundancy: minimal_with_failover
```

#### Single-User Optimization Plan

```toml
# fly-unified-api-vegas-optimized.toml
app = "sophia-api"
primary_region = "lax"  # Closest to Las Vegas

[deploy]
  strategy = "immediate"  # No rolling needed for single user
  max_unavailable = 1.0   # Can have full unavailability during updates

[regions]
  lax = 1  # 1 machine in Los Angeles (primary)
  sjc = 0  # Scale up to 1 for failover only

[[services]]
  protocol = "tcp"
  internal_port = 8003
  auto_stop_machines = true      # Critical for cost savings
  auto_start_machines = true     # Wake on demand
  min_machines_running = 0       # Allow complete scale to zero
  
  [[services.http_checks]]
    interval = "60s"  # Less frequent for single user
    grace_period = "10s"
    method = "GET"
    path = "/healthz"
    protocol = "http"
    timeout = "5s"
```

#### Cost-Optimized Single-User Configuration

```yaml
# Single user in Las Vegas optimization
optimization:
  location_specific:
    user_timezone: "America/Los_Angeles"
    peak_hours: "9am-6pm PST"
    
  scaling_strategy:
    business_hours:
      lax: 1  # 1 machine during work hours
    off_hours:
      lax: 0  # Scale to zero after hours
      
  wake_up_time: "< 2 seconds"  # Fast cold start
  
  estimated_monthly_cost:
    current: "$180/month"  # 24/7 operation
    optimized: "$45/month" # 9 hours/day operation
    savings: "$135/month" # 75% reduction
```

### 2. Lambda Labs GPU Integration

```python
# gpu_orchestrator.py
"""
GPU Workload Orchestrator for Lambda Labs Integration
"""

import asyncio
from typing import Dict, Any, Optional
import aiohttp
from dataclasses import dataclass
from enum import Enum

class GPUWorkloadType(Enum):
    TRAINING = "training"
    INFERENCE = "inference"
    BATCH_PROCESSING = "batch"
    REAL_TIME = "realtime"

@dataclass
class GPUConfig:
    instance_type: str = "gpu_1x_a100"  # A100 40GB
    region: str = "us-tx-1"  # Texas region
    max_instances: int = 5
    min_instances: int = 0
    auto_shutdown_minutes: int = 15

class LambdaLabsOrchestrator:
    """
    Orchestrates GPU workloads between Fly.io and Lambda Labs
    """
    
    def __init__(self):
        self.base_url = "https://cloud.lambdalabs.com/api/v1"
        self.api_key = os.environ.get("LAMBDA_LABS_API_KEY")
        self.active_instances = {}
        
    async def route_workload(self, workload: Dict[str, Any]) -> Dict[str, Any]:
        """Route workload to appropriate infrastructure"""
        
        workload_type = self._classify_workload(workload)
        
        if workload_type in [GPUWorkloadType.TRAINING, GPUWorkloadType.BATCH_PROCESSING]:
            # Route to Lambda Labs for GPU processing
            return await self._execute_on_lambda_labs(workload)
        else:
            # Keep on Fly.io for CPU workloads
            return await self._execute_on_fly(workload)
    
    async def _execute_on_lambda_labs(self, workload: Dict[str, Any]) -> Dict[str, Any]:
        """Execute workload on Lambda Labs GPU infrastructure"""
        
        # Provision GPU instance if needed
        instance_id = await self._provision_gpu_instance()
        
        # Execute workload
        result = await self._run_gpu_workload(instance_id, workload)
        
        # Auto-shutdown management
        asyncio.create_task(self._schedule_shutdown(instance_id))
        
        return result
    
    async def _provision_gpu_instance(self) -> str:
        """Provision Lambda Labs GPU instance"""
        
        async with aiohttp.ClientSession() as session:
            headers = {"Authorization": f"Bearer {self.api_key}"}
            
            payload = {
                "region_name": "us-tx-1",
                "instance_type_name": "gpu_1x_a100",
                "file_system_names": [],
                "ssh_key_names": ["sophia-artemis-key"],
                "quantity": 1
            }
            
            async with session.post(
                f"{self.base_url}/instance-operations/launch",
                headers=headers,
                json=payload
            ) as response:
                result = await response.json()
                instance_id = result["instance_ids"][0]
                
                self.active_instances[instance_id] = {
                    "status": "provisioning",
                    "created_at": datetime.now()
                }
                
                return instance_id
    
    def _classify_workload(self, workload: Dict[str, Any]) -> GPUWorkloadType:
        """Classify workload type for routing"""
        
        if "training" in workload.get("task_type", ""):
            return GPUWorkloadType.TRAINING
        elif "batch" in workload.get("task_type", ""):
            return GPUWorkloadType.BATCH_PROCESSING
        elif workload.get("requires_gpu", False):
            return GPUWorkloadType.INFERENCE
        else:
            return GPUWorkloadType.REAL_TIME
```

### 3. Blue-Green Deployment Strategy

```bash
#!/bin/bash
# blue_green_deploy.sh
# Blue-Green deployment script for Fly.io

set -e

APP_NAME="sophia-api"
BLUE_APP="${APP_NAME}-blue"
GREEN_APP="${APP_NAME}-green"

# Function to get active environment
get_active_env() {
    if fly status --app $BLUE_APP 2>/dev/null | grep -q "Deployed"; then
        echo "blue"
    else
        echo "green"
    fi
}

# Function to deploy to inactive environment
deploy_to_inactive() {
    local active=$(get_active_env)
    local target_app=""
    
    if [ "$active" = "blue" ]; then
        target_app=$GREEN_APP
        echo "üü¢ Deploying to GREEN environment..."
    else
        target_app=$BLUE_APP
        echo "üîµ Deploying to BLUE environment..."
    fi
    
    # Deploy to target environment
    fly deploy --app $target_app --config fly-unified-api.toml
    
    # Run health checks
    echo "üè• Running health checks..."
    for i in {1..30}; do
        if curl -f https://${target_app}.fly.dev/healthz; then
            echo "‚úÖ Health check passed"
            break
        fi
        sleep 10
    done
    
    # Switch traffic
    echo "üîÑ Switching traffic to new deployment..."
    fly ips list --app $APP_NAME | grep -v "VERSION" | awk '{print $1}' | while read ip; do
        fly ips allocate-v4 --app $target_app
        fly ips release $ip --app $active_app
    done
    
    echo "‚úÖ Blue-Green deployment complete!"
}

# Main deployment flow
deploy_to_inactive
```

### 4. Container Optimization

```dockerfile
# Optimized Dockerfile with multi-stage build
FROM python:3.11-slim as builder

# Install build dependencies
RUN apt-get update && apt-get install -y \
    gcc \
    g++ \
    && rm -rf /var/lib/apt/lists/*

# Copy requirements
COPY requirements.txt .

# Build wheels
RUN pip wheel --no-cache-dir --no-deps --wheel-dir /wheels -r requirements.txt

# Final stage
FROM python:3.11-slim

# Install runtime dependencies only
RUN apt-get update && apt-get install -y \
    curl \
    && rm -rf /var/lib/apt/lists/*

# Copy wheels and install
COPY --from=builder /wheels /wheels
RUN pip install --no-cache /wheels/*

# Copy application
WORKDIR /app
COPY . .

# Non-root user
RUN useradd -m -u 1001 sophia && chown -R sophia:sophia /app
USER sophia

# Health check
HEALTHCHECK --interval=30s --timeout=3s --start-period=5s --retries=3 \
    CMD curl -f http://localhost:8003/healthz || exit 1

CMD ["uvicorn", "app.api.unified_server:app", "--host", "0.0.0.0", "--port", "8003"]
```

### 5. Single-User Cost Optimization Strategy (Las Vegas)

```python
# vegas_cost_optimizer.py
"""
Single-User Cloud Cost Optimization Engine for Las Vegas Deployment
"""

class VegasCostOptimizer:
    """
    Optimizes cloud infrastructure costs for single user in Las Vegas
    """
    
    def __init__(self):
        self.fly_pricing = {
            "shared-cpu-1x": 0.0000008,  # per second ($2.07/month if 24/7)
            "shared-cpu-2x": 0.0000016,  # per second ($4.15/month if 24/7)
            "shared-cpu-4x": 0.0000032,  # per second ($8.30/month if 24/7)
            "performance-2x": 0.0000139  # per second ($36/month if 24/7)
        }
        
        self.lambda_labs_pricing = {
            "gpu_1x_a100": 1.29,  # per hour (only pay when used)
            "on_demand": True     # No reserved instances needed
        }
        
        self.user_profile = {
            "location": "Las Vegas, NV",
            "timezone": "PST",
            "work_hours": (9, 18),  # 9 AM to 6 PM
            "work_days": [1, 2, 3, 4, 5],  # Monday to Friday
            "usage_pattern": "development"  # Not production
        }
    
    async def optimize_single_user_deployment(self) -> Dict[str, Any]:
        """Optimize deployment for single Las Vegas user"""
        
        recommendations = {
            "immediate_actions": [],
            "scheduled_actions": [],
            "cost_breakdown": {},
            "total_savings": 0
        }
        
        # Immediate cost-saving actions
        recommendations["immediate_actions"] = [
            {
                "action": "Switch to LAX region",
                "reason": "Los Angeles is closer to Las Vegas (270 mi) than San Jose (370 mi)",
                "config": {
                    "primary_region": "lax",
                    "machines": 1,
                    "type": "shared-cpu-2x"  # Sufficient for single user
                },
                "impact": "10-15ms latency reduction"
            },
            {
                "action": "Enable aggressive auto-stop",
                "reason": "Single user doesn't need 24/7 availability",
                "config": {
                    "auto_stop_machines": True,
                    "auto_start_machines": True,
                    "min_machines_running": 0,
                    "idle_timeout": "5m"  # Stop after 5 minutes idle
                },
                "monthly_savings": "$135"
            },
            {
                "action": "Remove multi-region redundancy",
                "reason": "Single user doesn't need geographic distribution",
                "config": {
                    "regions_to_remove": ["iad", "lhr", "syd"],
                    "keep_regions": ["lax"]
                },
                "monthly_savings": "$240"
            }
        ]
        
        # Scheduled optimizations
        recommendations["scheduled_actions"] = [
            {
                "action": "Business hours scaling",
                "schedule": {
                    "scale_up": "0 9 * * 1-5",    # 9 AM weekdays
                    "scale_down": "0 18 * * 1-5"  # 6 PM weekdays
                },
                "estimated_uptime": "45 hours/week",
                "monthly_savings": "$95"
            },
            {
                "action": "Weekend shutdown",
                "schedule": {
                    "shutdown": "0 18 * * 5",  # Friday 6 PM
                    "startup": "0 9 * * 1"     # Monday 9 AM
                },
                "monthly_savings": "$40"
            }
        ]
        
        # GPU optimization for single user
        recommendations["gpu_strategy"] = {
            "approach": "on-demand only",
            "reasoning": "Single user needs GPU occasionally for training",
            "config": {
                "reserved_instances": 0,
                "spot_instances": True,
                "auto_terminate": "15 minutes after job completion"
            },
            "estimated_usage": "10 hours/month",
            "monthly_cost": "$13"  # vs $930 for reserved instance
        }
        
        # Cost breakdown
        recommendations["cost_breakdown"] = {
            "current": {
                "fly_io": "$180/month",  # Current multi-region setup
                "lambda_labs": "$0/month",  # Not utilized
                "total": "$180/month"
            },
            "optimized": {
                "fly_io": "$25/month",   # Single LAX instance, business hours only
                "lambda_labs": "$13/month",  # On-demand GPU when needed
                "total": "$38/month"
            },
            "monthly_savings": "$142",
            "annual_savings": "$1,704",
            "reduction_percentage": "79%"
        }
        
        return recommendations
    
    def generate_fly_config(self) -> str:
        """Generate optimized Fly.io configuration for Las Vegas user"""
        
        return """
# fly-vegas-optimized.toml
app = "sophia-api-vegas"
primary_region = "lax"  # Los Angeles - closest to Vegas

[build]
  dockerfile = "Dockerfile.slim"  # Use optimized single-user image

[env]
  DEPLOYMENT_MODE = "single_user"
  USER_LOCATION = "las_vegas"
  AUTO_SLEEP = "true"
  WAKE_ON_REQUEST = "true"

[services]
  protocol = "tcp"
  internal_port = 8003
  auto_stop_machines = true
  auto_start_machines = true
  min_machines_running = 0  # Allow complete shutdown
  
  [services.concurrency]
    type = "connections"
    hard_limit = 10  # Single user doesn't need high concurrency
    soft_limit = 5

[[vm]]
  cpu_kind = "shared"
  cpus = 2  # Reduced from 4
  memory_mb = 512  # Reduced from 4096

[schedule]
  # Auto-scale based on Las Vegas business hours
  scale_to_zero = "0 18 * * *"  # 6 PM daily
  scale_up = "0 9 * * 1-5"       # 9 AM weekdays only
"""
```

### 6. Monitoring and Observability Enhancement

```yaml
# monitoring_stack.yaml
monitoring:
  distributed_tracing:
    provider: datadog
    sampling_rate: 0.1
    
  metrics:
    collectors:
      - prometheus
      - cloudwatch
    export_interval: 60s
    
  logging:
    aggregator: elasticsearch
    retention_days: 30
    
  alerts:
    channels:
      - slack
      - pagerduty
    rules:
      - name: high_latency
        threshold: 500ms
        duration: 5m
      - name: error_rate
        threshold: 1%
        duration: 10m
      - name: gpu_utilization
        threshold: 90%
        duration: 15m
```

### 7. Infrastructure as Code Enhancement

```python
# pulumi_fly_integration.py
"""
Pulumi integration for Fly.io deployment
"""

import pulumi
import pulumi_command as command
from typing import Dict, List

class FlyDeployment(pulumi.ComponentResource):
    """
    Pulumi component for Fly.io deployment management
    """
    
    def __init__(self, name: str, config: Dict[str, Any], opts=None):
        super().__init__("sophia:fly:Deployment", name, None, opts)
        
        # Deploy to multiple regions
        for region, machine_count in config["regions"].items():
            self._deploy_region(name, region, machine_count, config)
        
        # Set up monitoring
        self._setup_monitoring(name, config)
        
        # Configure auto-scaling
        self._configure_autoscaling(name, config)
    
    def _deploy_region(self, app_name: str, region: str, machines: int, config: Dict):
        """Deploy app to specific region"""
        
        deploy_cmd = command.local.Command(
            f"{app_name}-deploy-{region}",
            create=f"""
                fly regions add {region} --app {app_name}
                fly scale count {machines} --region {region} --app {app_name}
            """,
            opts=pulumi.ResourceOptions(parent=self)
        )
        
        return deploy_cmd
    
    def _setup_monitoring(self, app_name: str, config: Dict):
        """Setup monitoring for Fly.io app"""
        
        monitoring_cmd = command.local.Command(
            f"{app_name}-monitoring",
            create=f"""
                fly metrics export datadog --app {app_name}
                fly logs ship elasticsearch --app {app_name}
            """,
            opts=pulumi.ResourceOptions(parent=self)
        )
        
        return monitoring_cmd
```

---

## üìà Implementation Roadmap

### Phase 1: Foundation (Week 1-2)
- [ ] Set up multi-region deployment on Fly.io
- [ ] Implement container optimization
- [ ] Configure basic cost monitoring

### Phase 2: GPU Integration (Week 3-4)
- [ ] Integrate Lambda Labs API
- [ ] Implement workload routing logic
- [ ] Set up GPU auto-scaling

### Phase 3: Advanced Deployment (Week 5-6)
- [ ] Implement blue-green deployment
- [ ] Set up canary deployments
- [ ] Configure automated rollback

### Phase 4: Monitoring & Optimization (Week 7-8)
- [ ] Deploy distributed tracing
- [ ] Implement cost optimization engine
- [ ] Set up comprehensive alerting

---

## üí∞ Expected Outcomes

### Performance Improvements (Single-User Las Vegas)
- **15ms latency** from LAX region (vs 25ms from SJC)
- **2-second cold starts** when waking from sleep
- **3x faster** ML workloads with on-demand Lambda Labs GPU
- **100% availability** during business hours (9 AM - 6 PM PST)

### Cost Optimizations (Single-User Las Vegas)
- **79% reduction** in infrastructure costs through:
  - Scale to zero during nights and weekends
  - Single region deployment (LAX only)
  - On-demand GPU usage only when needed
  - Reduced instance size (2 CPU/512MB vs 4 CPU/4GB)
- **$142/month savings** (from $180 to $38/month)
- **$1,704/year savings** with single-user optimization

### Operational Benefits
- **Zero-downtime deployments** with blue-green strategy
- **Automated failure recovery** with health checks and rollback
- **Real-time cost visibility** with monitoring dashboard

---

## üöÄ Quick Start Commands (Las Vegas Single-User)

```bash
# Switch to LAX region (closest to Las Vegas)
fly regions set lax --app sophia-api
fly regions remove sjc iad lhr syd --app sophia-api

# Configure for single user with aggressive auto-stop
fly scale count 1 --region lax --app sophia-api
fly autoscale set min=0 max=1 --app sophia-api

# Enable scale to zero with fast wake
fly scale vm shared-cpu-2x --memory 512 --app sophia-api

# Set up business hours scheduling (PST timezone)
fly ssh console --app sophia-api -C "crontab -l | { cat; echo '0 9 * * 1-5 fly scale count 1 --app sophia-api'; echo '0 18 * * * fly scale count 0 --app sophia-api'; } | crontab -"

# Monitor single-user costs
fly metrics costs --app sophia-api --json | jq '.monthly_estimate'

# On-demand GPU for training (only when needed)
python -m app.gpu.lambda_executor --workload training --instances 1 --auto-terminate 15
```

---

## üìä Monitoring Dashboard

Access deployment metrics and monitoring:
- Fly.io Dashboard: https://fly.io/apps/sophia-api
- Lambda Labs Console: https://cloud.lambdalabs.com/instances
- Cost Analytics: https://fly.io/organizations/sophia/billing

---

This optimization strategy transforms your deployment from a single-region setup to a globally distributed, cost-optimized, GPU-accelerated platform ready for production scale.