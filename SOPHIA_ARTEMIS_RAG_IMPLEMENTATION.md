# Sophia-Artemis Agentic RAG Implementation Plan
## Zero-Conflict, Zero-Debt Architecture

### Executive Summary
This implementation plan integrates advanced agentic RAG capabilities into the existing Sophia-Artemis ecosystem without creating conflicts, duplications, or technical debt. It builds upon the existing unified system while adding powerful memory and retrieval capabilities.

## 1. Architecture Overview

### 1.1 Service Architecture (No Conflicts)
```
Existing Services (Unchanged):
- Redis: 6379 (cache)
- PostgreSQL: 5432 (optional)
- MCP Memory: 8765 (existing)
- MCP Bridge: 8766 (existing)
- Sophia Backend: 8000

New RAG Services (Non-Conflicting):
- Sophia BI Memory: 8767
- Artemis Code Memory: 8768
- Vector Store (Weaviate): 8080
- Graph Store (Neo4j): 7474, 7687 (optional)
```

### 1.2 Domain Separation
```yaml
sophia_domain:
  purpose: Business Intelligence
  port: 8767
  data_types:
    - customer_analytics
    - sales_metrics
    - business_reports
    - service_integrations
  api_keys:
    - SALESFORCE_CLIENT_ID
    - HUBSPOT_API_KEY
    - ASANA_API_TOKEN
    - LINEAR_API_KEY
    - SLACK_API_TOKEN

artemis_domain:
  purpose: Code Intelligence
  port: 8768
  data_types:
    - code_repositories
    - documentation
    - test_suites
    - deployment_configs
  api_keys:
    - OPENAI_API_KEY
    - ANTHROPIC_API_KEY
    - GROQ_API_KEY
    - GROK_API_KEY
```

## 2. Implementation Files

### 2.1 Enhanced Startup Script Extension
```bash
# Addition to existing start.sh (not replacement)
# Add after existing services in start.sh

# RAG Services (optional, graceful degradation)
if should_start "rag-services"; then
    # Weaviate Vector Store
    if [ -f "docker-compose.rag.yml" ]; then
        docker-compose -f docker-compose.rag.yml up -d weaviate
        wait_for_service "weaviate" "8080"
    fi
    
    # Sophia BI Memory
    start_service "sophia-memory" \
        "python3 app/memory/sophia_memory.py --port 8767" \
        "8767"
    
    # Artemis Code Memory
    start_service "artemis-memory" \
        "python3 app/memory/artemis_memory.py --port 8768" \
        "8768"
fi
```

### 2.2 Domain Router Implementation
```python
# app/agents/domain_router.py
"""
Domain Router for Sophia-Artemis RAG
Routes queries to appropriate domain based on content
"""

from typing import Dict, Any, Optional, Literal
from enum import Enum

class Domain(Enum):
    SOPHIA = "sophia"  # Business Intelligence
    ARTEMIS = "artemis"  # Code Intelligence
    UNIFIED = "unified"  # Cross-domain

class DomainRouter:
    """Routes queries to appropriate domain"""
    
    # Domain keywords for auto-routing
    SOPHIA_KEYWORDS = {
        "revenue", "sales", "customer", "metric", "kpi", 
        "business", "report", "dashboard", "analytics",
        "slack", "asana", "hubspot", "salesforce"
    }
    
    ARTEMIS_KEYWORDS = {
        "code", "function", "class", "bug", "test",
        "deploy", "refactor", "api", "implement", "debug",
        "repository", "git", "python", "javascript"
    }
    
    @classmethod
    def detect_domain(cls, query: str) -> Domain:
        """Auto-detect domain from query content"""
        query_lower = query.lower()
        
        sophia_score = sum(1 for kw in cls.SOPHIA_KEYWORDS if kw in query_lower)
        artemis_score = sum(1 for kw in cls.ARTEMIS_KEYWORDS if kw in query_lower)
        
        if sophia_score > 0 and artemis_score > 0:
            return Domain.UNIFIED
        elif sophia_score > artemis_score:
            return Domain.SOPHIA
        elif artemis_score > sophia_score:
            return Domain.ARTEMIS
        else:
            return Domain.UNIFIED
    
    @classmethod
    def get_memory_endpoint(cls, domain: Domain) -> str:
        """Get memory service endpoint for domain"""
        endpoints = {
            Domain.SOPHIA: "http://localhost:8767",
            Domain.ARTEMIS: "http://localhost:8768",
            Domain.UNIFIED: "http://localhost:8765"
        }
        return endpoints.get(domain, "http://localhost:8765")
```

### 2.3 Base Memory Service
```python
# app/memory/base_memory.py
"""
Base Memory Service for RAG Implementation
Shared by both Sophia and Artemis domains
"""

import asyncio
import json
from typing import List, Dict, Any, Optional
from datetime import datetime
from abc import ABC, abstractmethod

import httpx
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
import redis
from weaviate import Client as WeaviateClient

class MemoryQuery(BaseModel):
    query: str
    domain: Optional[str] = None
    limit: int = 10
    include_context: bool = True

class MemoryResponse(BaseModel):
    results: List[Dict[str, Any]]
    domain: str
    timestamp: datetime
    context_used: bool

class BaseMemoryService(ABC):
    """Base class for domain-specific memory services"""
    
    def __init__(self, domain: str, port: int):
        self.domain = domain
        self.port = port
        self.app = FastAPI(title=f"{domain.title()} Memory Service")
        
        # Initialize storage backends
        self.redis_client = redis.Redis(host='localhost', port=6379, decode_responses=True)
        self.weaviate_client = None  # Lazy init
        
        self._setup_routes()
    
    def _setup_routes(self):
        """Setup FastAPI routes"""
        
        @self.app.get("/health")
        async def health():
            return {"status": "healthy", "domain": self.domain}
        
        @self.app.post("/query", response_model=MemoryResponse)
        async def query_memory(request: MemoryQuery):
            results = await self.search(request.query, request.limit)
            
            if request.include_context:
                results = await self.enrich_with_context(results)
            
            return MemoryResponse(
                results=results,
                domain=self.domain,
                timestamp=datetime.now(),
                context_used=request.include_context
            )
        
        @self.app.post("/index")
        async def index_document(document: Dict[str, Any]):
            success = await self.index(document)
            return {"success": success, "domain": self.domain}
    
    @abstractmethod
    async def search(self, query: str, limit: int) -> List[Dict[str, Any]]:
        """Domain-specific search implementation"""
        pass
    
    @abstractmethod
    async def index(self, document: Dict[str, Any]) -> bool:
        """Domain-specific indexing implementation"""
        pass
    
    @abstractmethod
    async def enrich_with_context(self, results: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """Add domain-specific context to results"""
        pass
    
    def run(self):
        """Start the memory service"""
        import uvicorn
        uvicorn.run(self.app, host="0.0.0.0", port=self.port)
```

### 2.4 Sophia BI Memory Service
```python
# app/memory/sophia_memory.py
"""
Sophia Business Intelligence Memory Service
Handles BI-specific context and retrieval
"""

from typing import List, Dict, Any
from app.memory.base_memory import BaseMemoryService

class SophiaMemoryService(BaseMemoryService):
    """Memory service for business intelligence domain"""
    
    def __init__(self):
        super().__init__(domain="sophia", port=8767)
        self.collection_name = "business_intelligence"
    
    async def search(self, query: str, limit: int) -> List[Dict[str, Any]]:
        """Search business intelligence context"""
        # Check Redis cache first
        cache_key = f"sophia:query:{query[:50]}"
        cached = self.redis_client.get(cache_key)
        if cached:
            return json.loads(cached)
        
        results = []
        
        # Search in Weaviate if available
        if self.weaviate_client:
            try:
                weaviate_results = (
                    self.weaviate_client.query
                    .get(self.collection_name, ["content", "metadata", "source"])
                    .with_hybrid(query=query, alpha=0.5)
                    .with_limit(limit)
                    .do()
                )
                
                if self.collection_name in weaviate_results.get("data", {}).get("Get", {}):
                    results = weaviate_results["data"]["Get"][self.collection_name]
            except Exception as e:
                print(f"Weaviate search error: {e}")
        
        # Fallback to keyword search in Redis
        if not results:
            pattern = f"sophia:doc:*{query.lower()}*"
            for key in self.redis_client.scan_iter(match=pattern, count=100):
                doc = self.redis_client.get(key)
                if doc:
                    results.append(json.loads(doc))
                    if len(results) >= limit:
                        break
        
        # Cache results
        if results:
            self.redis_client.setex(cache_key, 3600, json.dumps(results))
        
        return results
    
    async def index(self, document: Dict[str, Any]) -> bool:
        """Index business document"""
        try:
            # Store in Redis for fast access
            doc_id = document.get("id", f"sophia:{datetime.now().timestamp()}")
            self.redis_client.setex(
                f"sophia:doc:{doc_id}",
                86400,  # 24 hour TTL
                json.dumps(document)
            )
            
            # Index in Weaviate if available
            if self.weaviate_client:
                self.weaviate_client.data_object.create(
                    document,
                    self.collection_name
                )
            
            return True
        except Exception as e:
            print(f"Indexing error: {e}")
            return False
    
    async def enrich_with_context(self, results: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """Add business context to results"""
        for result in results:
            # Add business metrics context
            if "customer" in str(result.get("content", "")).lower():
                result["context"] = {
                    "related_metrics": ["customer_lifetime_value", "churn_rate", "nps_score"],
                    "data_sources": ["salesforce", "hubspot", "analytics"]
                }
            
            # Add temporal context
            result["temporal_context"] = {
                "quarter": "Q1 2025",
                "fiscal_year": "FY2025"
            }
        
        return results

if __name__ == "__main__":
    service = SophiaMemoryService()
    service.run()
```

### 2.5 Artemis Code Memory Service
```python
# app/memory/artemis_memory.py
"""
Artemis Code Intelligence Memory Service
Handles code-specific context and retrieval
"""

from typing import List, Dict, Any
import ast
from app.memory.base_memory import BaseMemoryService

class ArtemisMemoryService(BaseMemoryService):
    """Memory service for code intelligence domain"""
    
    def __init__(self):
        super().__init__(domain="artemis", port=8768)
        self.collection_name = "code_intelligence"
    
    async def search(self, query: str, limit: int) -> List[Dict[str, Any]]:
        """Search code context"""
        # Check Redis cache
        cache_key = f"artemis:query:{query[:50]}"
        cached = self.redis_client.get(cache_key)
        if cached:
            return json.loads(cached)
        
        results = []
        
        # Search in Weaviate if available
        if self.weaviate_client:
            try:
                weaviate_results = (
                    self.weaviate_client.query
                    .get(self.collection_name, ["code", "language", "filepath", "description"])
                    .with_hybrid(query=query, alpha=0.7)  # More semantic for code
                    .with_limit(limit)
                    .do()
                )
                
                if self.collection_name in weaviate_results.get("data", {}).get("Get", {}):
                    results = weaviate_results["data"]["Get"][self.collection_name]
            except Exception as e:
                print(f"Weaviate search error: {e}")
        
        # Fallback to pattern matching in Redis
        if not results:
            pattern = f"artemis:code:*{query.lower()}*"
            for key in self.redis_client.scan_iter(match=pattern, count=100):
                doc = self.redis_client.get(key)
                if doc:
                    results.append(json.loads(doc))
                    if len(results) >= limit:
                        break
        
        # Cache results
        if results:
            self.redis_client.setex(cache_key, 3600, json.dumps(results))
        
        return results
    
    async def index(self, document: Dict[str, Any]) -> bool:
        """Index code document"""
        try:
            # Extract code metadata if present
            if "code" in document:
                document["metadata"] = self._extract_code_metadata(document["code"])
            
            # Store in Redis
            doc_id = document.get("id", f"artemis:{datetime.now().timestamp()}")
            self.redis_client.setex(
                f"artemis:code:{doc_id}",
                86400,  # 24 hour TTL
                json.dumps(document)
            )
            
            # Index in Weaviate if available
            if self.weaviate_client:
                self.weaviate_client.data_object.create(
                    document,
                    self.collection_name
                )
            
            return True
        except Exception as e:
            print(f"Indexing error: {e}")
            return False
    
    def _extract_code_metadata(self, code: str) -> Dict[str, Any]:
        """Extract metadata from code"""
        metadata = {
            "lines": len(code.splitlines()),
            "has_classes": "class " in code,
            "has_functions": "def " in code,
            "imports": []
        }
        
        try:
            tree = ast.parse(code)
            for node in ast.walk(tree):
                if isinstance(node, ast.Import):
                    for alias in node.names:
                        metadata["imports"].append(alias.name)
                elif isinstance(node, ast.ImportFrom):
                    metadata["imports"].append(node.module)
        except:
            pass
        
        return metadata
    
    async def enrich_with_context(self, results: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """Add code context to results"""
        for result in results:
            # Add language context
            code = result.get("code", "")
            result["language_context"] = {
                "language": result.get("language", "python"),
                "complexity": "low" if len(code) < 100 else "medium" if len(code) < 500 else "high"
            }
            
            # Add related files context
            if "filepath" in result:
                result["related_files"] = {
                    "test_file": result["filepath"].replace(".py", "_test.py"),
                    "doc_file": result["filepath"].replace(".py", ".md")
                }
        
        return results

if __name__ == "__main__":
    service = ArtemisMemoryService()
    service.run()
```

### 2.6 Enhanced Unified AI Agent CLI
```python
# Addition to scripts/unified_ai_agents.py
# Add this to the existing UnifiedAgentCLI class

def __init__(self):
    # ... existing code ...
    
    # Add domain router
    from app.agents.domain_router import DomainRouter, Domain
    self.domain_router = DomainRouter()
    
    # RAG endpoints
    self.rag_endpoints = {
        Domain.SOPHIA: "http://localhost:8767",
        Domain.ARTEMIS: "http://localhost:8768",
        Domain.UNIFIED: "http://localhost:8765"
    }

async def execute_task_with_rag(self, args):
    """Execute task with RAG context enhancement"""
    
    # Detect domain if not specified
    if not args.domain:
        domain = self.domain_router.detect_domain(args.task)
    else:
        domain = Domain[args.domain.upper()]
    
    # Get RAG context
    rag_context = await self.get_rag_context(args.task, domain)
    
    # Enhance task with context
    if rag_context:
        enhanced_task = {
            "original_task": args.task,
            "domain": domain.value,
            "context": rag_context,
            "timestamp": datetime.now().isoformat()
        }
        args.task = self.format_with_context(args.task, rag_context)
    
    # Execute with existing logic
    return await self.execute_task(args)

async def get_rag_context(self, query: str, domain: Domain) -> Dict[str, Any]:
    """Retrieve RAG context from appropriate domain"""
    endpoint = self.rag_endpoints.get(domain)
    
    try:
        async with httpx.AsyncClient() as client:
            response = await client.post(
                f"{endpoint}/query",
                json={"query": query, "limit": 5}
            )
            
            if response.status_code == 200:
                return response.json()
    except:
        # Graceful degradation if RAG service unavailable
        pass
    
    return None

def format_with_context(self, task: str, context: Dict[str, Any]) -> str:
    """Format task with RAG context"""
    if not context or not context.get("results"):
        return task
    
    # Build context string
    context_str = "\n\nRelevant context:\n"
    for i, result in enumerate(context["results"][:3], 1):
        content = result.get("content", result.get("code", ""))
        if content:
            context_str += f"{i}. {content[:200]}...\n"
    
    return f"{task}{context_str}"
```

## 3. Docker Compose for RAG Services

```yaml
# docker-compose.rag.yml
version: '3.8'

services:
  weaviate:
    image: semitechnologies/weaviate:latest
    restart: unless-stopped
    ports:
      - "8080:8080"
    environment:
      QUERY_DEFAULTS_LIMIT: 25
      AUTHENTICATION_ANONYMOUS_ACCESS_ENABLED: 'true'
      PERSISTENCE_DATA_PATH: '/var/lib/weaviate'
      DEFAULT_VECTORIZER_MODULE: 'text2vec-transformers'
      ENABLE_MODULES: 'text2vec-transformers'
      TRANSFORMERS_INFERENCE_API: 'http://t2v-transformers:8080'
    volumes:
      - weaviate_data:/var/lib/weaviate

  t2v-transformers:
    image: semitechnologies/transformers-inference:sentence-transformers-multi-qa-MiniLM-L6-cos-v1
    environment:
      ENABLE_CUDA: '0'  # CPU only for Mac compatibility

  neo4j:
    image: neo4j:5-community
    restart: unless-stopped
    ports:
      - "7474:7474"
      - "7687:7687"
    environment:
      NEO4J_AUTH: neo4j/sophia2025
      NEO4J_dbms_memory_pagecache_size: 1G
      NEO4J_dbms_memory_heap_max__size: 1G
    volumes:
      - neo4j_data:/data
      - neo4j_logs:/logs

volumes:
  weaviate_data:
  neo4j_data:
  neo4j_logs:
```

## 4. Migration Script

```python
# scripts/migrate_to_rag.py
#!/usr/bin/env python3
"""
Migrate existing data to RAG memory services
Zero-conflict migration preserving existing functionality
"""

import asyncio
import json
from pathlib import Path
import httpx

async def migrate_to_rag():
    """Migrate existing context to domain-specific RAG services"""
    
    print("üöÄ Starting RAG Migration (Non-Destructive)")
    
    # Check services are running
    services = {
        "Sophia Memory": "http://localhost:8767/health",
        "Artemis Memory": "http://localhost:8768/health"
    }
    
    for name, url in services.items():
        try:
            async with httpx.AsyncClient() as client:
                response = await client.get(url)
                if response.status_code == 200:
                    print(f"‚úÖ {name} is running")
                else:
                    print(f"‚ùå {name} is not healthy")
                    return
        except:
            print(f"‚ùå {name} is not reachable at {url}")
            print("   Start with: ./start.sh --services rag-services")
            return
    
    # Migrate business documents to Sophia
    print("\nüìä Migrating business documents to Sophia...")
    business_docs = [
        {"type": "metric", "content": "Q4 revenue increased by 15%", "source": "salesforce"},
        {"type": "report", "content": "Customer satisfaction score: 4.5/5", "source": "hubspot"},
        # Add your actual business documents
    ]
    
    async with httpx.AsyncClient() as client:
        for doc in business_docs:
            response = await client.post(
                "http://localhost:8767/index",
                json=doc
            )
            if response.status_code == 200:
                print(f"  ‚úÖ Indexed: {doc['type']}")
    
    # Migrate code documents to Artemis
    print("\nüíª Migrating code documents to Artemis...")
    
    # Find Python files
    for py_file in Path(".").glob("**/*.py"):
        if "venv" in str(py_file) or "__pycache__" in str(py_file):
            continue
        
        try:
            code = py_file.read_text()
            doc = {
                "filepath": str(py_file),
                "language": "python",
                "code": code[:5000],  # Limit size
                "description": f"Code from {py_file.name}"
            }
            
            async with httpx.AsyncClient() as client:
                response = await client.post(
                    "http://localhost:8768/index",
                    json=doc
                )
                if response.status_code == 200:
                    print(f"  ‚úÖ Indexed: {py_file}")
        except Exception as e:
            print(f"  ‚ö†Ô∏è  Skipped {py_file}: {e}")
    
    print("\nüéâ Migration complete!")

if __name__ == "__main__":
    asyncio.run(migrate_to_rag())
```

## 5. Testing & Validation

```bash
# scripts/test_rag_integration.sh
#!/bin/bash
# Test RAG integration without conflicts

source scripts/env.sh

echo "üß™ Testing RAG Integration"

# 1. Test domain routing
echo -e "\nüìç Testing Domain Routing..."
python3 -c "
from app.agents.domain_router import DomainRouter
test_queries = {
    'Show me Q4 revenue metrics': 'sophia',
    'Debug the authentication function': 'artemis',
    'How did code changes affect sales?': 'unified'
}
for query, expected in test_queries.items():
    domain = DomainRouter.detect_domain(query)
    print(f'  {domain.value == expected} {query} -> {domain.value}')
"

# 2. Test memory services
echo -e "\nüß† Testing Memory Services..."
for port in 8767 8768; do
    response=$(curl -s http://localhost:$port/health)
    if [[ $response == *"healthy"* ]]; then
        echo "  ‚úÖ Service on port $port is healthy"
    else
        echo "  ‚ùå Service on port $port is not responding"
    fi
done

# 3. Test unified CLI with RAG
echo -e "\nü§ñ Testing Unified CLI with RAG..."
python3 scripts/unified_ai_agents.py --agent grok --task "analyze customer churn" --dry-run
python3 scripts/unified_ai_agents.py --agent claude --task "refactor the auth module" --dry-run

echo -e "\n‚úÖ RAG Integration tests complete"
```

## 6. Documentation Updates

### 6.1 Main README Addition
```markdown
## RAG Memory Services (Optional Enhancement)

The system includes optional RAG (Retrieval-Augmented Generation) services for enhanced context:

### Starting RAG Services
```bash
# Start with RAG services
./start.sh --services all,rag-services

# Or start only RAG
./start.sh --services rag-services
```

### Domain-Specific Memory
- **Sophia BI Memory (8767)**: Business intelligence context
- **Artemis Code Memory (8768)**: Code intelligence context

### Using RAG with AI Agents
```bash
# Automatic domain detection
python3 scripts/unified_ai_agents.py --agent grok --task "analyze Q4 revenue"

# Explicit domain selection  
python3 scripts/unified_ai_agents.py --agent claude --task "fix bug" --domain artemis
```
```

### 6.2 API Documentation
Create `docs/RAG_API_REFERENCE.md`:

```markdown
# RAG API Reference

## Sophia BI Memory API (Port 8767)

### POST /query
Query business intelligence context

Request:
```json
{
  "query": "revenue metrics",
  "limit": 10,
  "include_context": true
}
```

### POST /index
Index business document

Request:
```json
{
  "type": "metric",
  "content": "Q4 revenue data",
  "source": "salesforce",
  "metadata": {}
}
```

## Artemis Code Memory API (Port 8768)

### POST /query
Query code intelligence context

Request:
```json
{
  "query": "authentication function",
  "limit": 5,
  "include_context": true
}
```

### POST /index
Index code document

Request:
```json
{
  "filepath": "app/auth.py",
  "language": "python",
  "code": "def authenticate()...",
  "description": "Auth module"
}
```
```

## 7. Implementation Checklist

### Phase 1: Foundation (Week 1)
- [ ] Add RAG service extensions to start.sh
- [ ] Create base memory service (app/memory/base_memory.py)
- [ ] Implement domain router (app/agents/domain_router.py)
- [ ] Deploy docker-compose.rag.yml for vector stores

### Phase 2: Domain Services (Week 2)
- [ ] Implement Sophia BI memory service
- [ ] Implement Artemis Code memory service
- [ ] Enhance unified CLI with RAG context
- [ ] Create migration script

### Phase 3: Testing & Documentation (Week 3)
- [ ] Run comprehensive testing suite
- [ ] Update all documentation
- [ ] Create usage examples
- [ ] Performance benchmarking

### Phase 4: Optimization (Week 4)
- [ ] Tune vector search parameters
- [ ] Optimize caching strategies
- [ ] Add monitoring and metrics
- [ ] Create backup/recovery procedures

## 8. Key Success Metrics

### No Conflicts
- ‚úÖ All services use unique ports
- ‚úÖ No virtual environment creation
- ‚úÖ Extends existing systems without breaking changes
- ‚úÖ Backward compatible with existing CLI

### Performance Targets
- Query latency: <200ms with cache, <2s without
- Indexing throughput: >100 docs/second
- Memory usage: <2GB per service
- Uptime: 99.9% with graceful degradation

### Business Impact
- 40% reduction in context switching for developers
- 60% improvement in relevant context retrieval
- 50% decrease in time to find related code/documents
- Zero increase in operational complexity

## 9. Conclusion

This implementation plan provides a conflict-free integration of advanced RAG capabilities into the Sophia-Artemis ecosystem. It:

1. **Preserves existing functionality** - No breaking changes
2. **Maintains separation** - Clear domain boundaries
3. **Enables graceful degradation** - System works without RAG
4. **Provides clear value** - Measurable improvements in context retrieval
5. **Follows best practices** - Zero technical debt, comprehensive documentation

The phased approach ensures smooth implementation with minimal risk, while the comprehensive testing and documentation ensure long-term maintainability.