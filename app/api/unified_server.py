"""
Unified Agent API Server.
Consolidates all agent endpoints, MCP integration, and retrieval systems.
Eliminates fragmentation between shim servers and provides a single gateway.
"""

from fastapi import FastAPI, HTTPException, BackgroundTasks
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import StreamingResponse, JSONResponse
from pydantic import BaseModel, Field
from typing import List, Dict, Any, Optional, AsyncGenerator
import asyncio
import json
import os
from datetime import datetime
from contextlib import asynccontextmanager

# Import our enhanced systems
from app.memory.supermemory_mcp import SupermemoryStore, MemoryEntry, MemoryType
from app.memory.dual_tier_embeddings import DualTierEmbedder
from app.memory.hybrid_search import HybridSearchEngine
from app.memory.graph_rag import KnowledgeGraph, GraphRAGEngine
from app.evaluation.gates import EvaluationGateManager
from app.contracts.json_schemas import (
    validate_critic_output,
    validate_judge_output,
    runner_gate_decision
)
from app.portkey_config import gateway, Role, MODEL_RECOMMENDATIONS
# Import swarm execution system
from app.swarms.unified_enhanced_orchestrator import UnifiedSwarmOrchestrator
from app.api.real_swarm_execution import stream_real_swarm_execution

# ============================================
# Configuration
# ============================================

class ServerConfig:
    """Unified server configuration."""
    
    # Server
    HOST = "0.0.0.0"
    PORT = int(os.getenv("AGENT_API_PORT", "8000"))
    
    # CORS
    ALLOWED_ORIGINS = [
        "http://localhost:3000",
        "http://localhost:3002",
        "http://localhost:7777"
    ]
    
    # Playground
    PLAYGROUND_URL = os.getenv("PLAYGROUND_URL", "http://localhost:7777")
    
    # üîß LOCAL DEVELOPMENT MODE - ENABLES ALL TOOLS
    LOCAL_DEV_MODE = os.getenv("LOCAL_DEV_MODE", "true").lower() == "true"
    ENABLE_RUNNER_WRITES = os.getenv("ENABLE_RUNNER_WRITES", str(LOCAL_DEV_MODE)).lower() == "true"
    RUNNER_GATE_OVERRIDE = LOCAL_DEV_MODE  # Allow Runner in dev mode
    
    # MCP Servers
    MCP_FILESYSTEM_ENABLED = os.getenv("MCP_FILESYSTEM", "true").lower() == "true"
    MCP_GIT_ENABLED = os.getenv("MCP_GIT", "true").lower() == "true"
    MCP_SUPERMEMORY_ENABLED = os.getenv("MCP_SUPERMEMORY", "true").lower() == "true"
    
    # Features
    GRAPHRAG_ENABLED = os.getenv("GRAPHRAG_ENABLED", "true").lower() == "true"
    HYBRID_SEARCH_ENABLED = os.getenv("HYBRID_SEARCH", "true").lower() == "true"
    GATES_ENABLED = os.getenv("EVALUATION_GATES", "true").lower() == "true"
    
    @classmethod
    def print_config(cls):
        """Print current configuration."""
        if cls.LOCAL_DEV_MODE:
            print("\n" + "="*60)
            print("üîß LOCAL DEVELOPMENT MODE ACTIVE")
            print("="*60)
            print("‚úÖ Runner writes: ENABLED")
            print("‚úÖ Git operations: ENABLED")
            print("‚úÖ File operations: ENABLED")
            print("‚úÖ All tools: ACTIVE")
            print("‚ö†Ô∏è  Be careful - all write operations are enabled!")
            print("="*60 + "\n")

# ============================================
# Global State
# ============================================

class GlobalState:
    """Manages global application state."""
    
    def __init__(self):
        self.supermemory = None
        self.embedder = None
        self.search_engine = None
        self.knowledge_graph = None
        self.graph_rag = None
        self.gate_manager = None
        self.orchestrator = None
        self.initialized = False
    
    async def initialize(self):
        """Initialize all systems."""
        if self.initialized:
            return
        
        print("üöÄ Initializing unified agent systems...")
        
        # Initialize Supermemory
        if ServerConfig.MCP_SUPERMEMORY_ENABLED:
            self.supermemory = SupermemoryStore()
            print("  ‚úÖ Supermemory MCP initialized")
        
        # Initialize ModernBERT embedder (2025 SOTA)
        from app.memory.modernbert_embeddings import ModernBERTEmbedder
        self.embedder = ModernBERTEmbedder()
        print("  ‚úÖ ModernBERT embedder initialized (Voyage-3-large/Cohere v3)")
        
        # Initialize search engine
        if ServerConfig.HYBRID_SEARCH_ENABLED:
            self.search_engine = HybridSearchEngine(embedder=self.embedder)
            print("  ‚úÖ Hybrid search engine initialized")
        
        # Initialize GraphRAG
        if ServerConfig.GRAPHRAG_ENABLED:
            self.knowledge_graph = KnowledgeGraph()
            self.graph_rag = GraphRAGEngine(self.knowledge_graph)
            print("  ‚úÖ GraphRAG system initialized")
        
        # Initialize evaluation gates
        if ServerConfig.GATES_ENABLED:
            self.gate_manager = EvaluationGateManager()
            print("  ‚úÖ Evaluation gates initialized")
        
        # Initialize orchestrator for real swarm execution
        self.orchestrator = UnifiedSwarmOrchestrator()
        print("  ‚úÖ Swarm orchestrator initialized")
        
        self.initialized = True
        print("‚úÖ All systems initialized successfully")
    
    async def cleanup(self):
        """Cleanup resources."""
        print("üßπ Cleaning up resources...")
        self.initialized = False

state = GlobalState()

# ============================================
# Lifespan Management
# ============================================

@asynccontextmanager
async def lifespan(app: FastAPI):
    """Manage application lifespan."""
    # Startup
    await state.initialize()
    
    # Register MCP servers (simulation - in production, use actual MCP client)
    if ServerConfig.MCP_FILESYSTEM_ENABLED:
        print("üìÅ MCP Filesystem server registered")
    if ServerConfig.MCP_GIT_ENABLED:
        print("üîÄ MCP Git server registered")
    if ServerConfig.MCP_SUPERMEMORY_ENABLED:
        print("üß† MCP Supermemory server registered")
    
    yield
    
    # Shutdown
    await state.cleanup()

# ============================================
# FastAPI App
# ============================================

app = FastAPI(
    title="Unified Agent API",
    description="Consolidated API for all agent operations with MCP, retrieval, and gates",
    version="2.0.0",
    lifespan=lifespan
)

# CORS middleware
app.add_middleware(
    CORSMiddleware,
    allow_origins=ServerConfig.ALLOWED_ORIGINS,
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# ============================================
# Request/Response Models
# ============================================

class TeamInfo(BaseModel):
    """Team information."""
    id: str
    name: str
    description: str
    members: List[str] = Field(default_factory=list)
    model_pool: str = "balanced"

class WorkflowInfo(BaseModel):
    """Workflow information."""
    id: str
    name: str
    description: str
    inputs: Dict[str, str] = Field(default_factory=dict)
    steps: List[str] = Field(default_factory=list)

class RunRequest(BaseModel):
    """Request to run team or workflow."""
    team_id: Optional[str] = None
    workflow_id: Optional[str] = None
    message: str
    additional_data: Optional[Dict[str, Any]] = None
    pool: str = "balanced"  # fast/heavy/balanced
    use_graphrag: bool = False
    use_memory: bool = True

class MemoryRequest(BaseModel):
    """Memory add request."""
    topic: str
    content: str
    source: str
    tags: List[str] = Field(default_factory=list)
    memory_type: str = "semantic"

class SearchRequest(BaseModel):
    """Search request."""
    query: str
    top_k: int = 10
    use_semantic: bool = True
    use_bm25: bool = True
    use_reranker: bool = True
    include_graph: bool = False

# ============================================
# Health & Discovery Endpoints
# ============================================

@app.get("/healthz")
async def health():
    """Health check with system status."""
    return {
        "status": "ok",
        "timestamp": datetime.now().isoformat(),
        "systems": {
            "supermemory": state.supermemory is not None,
            "embedder": state.embedder is not None,
            "search": state.search_engine is not None,
            "graphrag": state.graph_rag is not None,
            "gates": state.gate_manager is not None
        }
    }

@app.get("/config")
async def get_config():
    """Get runtime configuration (dev mode only)."""
    # Only allow in development mode
    if not os.getenv("LOCAL_DEV_MODE", "false").lower() == "true":
        raise HTTPException(status_code=403, detail="Config endpoint only available in dev mode")
    
    return {
        "environment": "development" if os.getenv("LOCAL_DEV_MODE") else "production",
        "server": {
            "api_port": os.getenv("AGENT_API_PORT", "8003"),
            "ui_port": os.getenv("AGENT_UI_PORT", "3000"),
            "allowed_origins": os.getenv("ALLOWED_ORIGINS", "http://localhost:3000").split(","),
            "rate_limit": os.getenv("RATE_LIMIT_PER_MINUTE", "60")
        },
        "features": {
            "mcp_servers": {
                "filesystem": os.getenv("ENABLE_MCP_FILESYSTEM", "true").lower() == "true",
                "git": os.getenv("ENABLE_MCP_GIT", "true").lower() == "true",
                "supermemory": os.getenv("ENABLE_MCP_SUPERMEMORY", "true").lower() == "true"
            },
            "evaluation_gates": ["security", "accuracy", "consistency", "safety"],
            "swarm_patterns": ["adversarial_debate", "quality_gates", "consensus", "dynamic_roles"],
            "model_pools": ["premium", "balanced", "free"]
        },
        "models": {
            "provider": "openrouter",
            "available_count": 499,
            "latest_models": [
                "openai/gpt-5",
                "anthropic/claude-4",
                "google/gemini-2.5-pro",
                "deepseek/deepseek-r1",
                "x-ai/grok-code-fast-1"
            ],
            "fallback_chain": ["primary", "secondary", "free"],
            "default_pool": os.getenv("DEFAULT_MODEL_POOL", "balanced")
        },
        "database": {
            "memory_backend": "supermemory" if not os.getenv("DATABASE_URL") else "postgresql",
            "vector_store": "in-memory" if not os.getenv("FAISS_INDEX_PATH") else "faiss",
            "cache": "none" if not os.getenv("REDIS_URL") else "redis"
        },
        "security": {
            "authentication": "none" if not os.getenv("JWT_SECRET") else "jwt",
            "rate_limiting": os.getenv("ENABLE_RATE_LIMIT", "false").lower() == "true",
            "cors_enabled": True,
            "audit_logging": os.getenv("ENABLE_AUDIT_LOG", "false").lower() == "true"
        }
    }

@app.get("/teams", response_model=List[TeamInfo])
async def get_teams():
    """Get available AI swarms with specialized capabilities."""
    return [
        TeamInfo(
            id="strategic-swarm",
            name="Strategic Planning Swarm",
            description="High-level strategy, architecture, and product planning swarm with enterprise focus",
            members=[
                "Chief Architect", 
                "Strategic Planner", 
                "Product Manager", 
                "Technical Lead", 
                "Systems Analyst", 
                "Business Analyst"
            ],
            model_pool="premium"
        ),
        TeamInfo(
            id="development-swarm",
            name="Development & Implementation Swarm", 
            description="Core development swarm for coding, implementation, and feature building",
            members=[
                "Lead Developer",
                "Senior Engineer A",
                "Senior Engineer B", 
                "Full-Stack Developer",
                "DevOps Engineer",
                "Code Reviewer"
            ],
            model_pool="balanced"
        ),
        TeamInfo(
            id="security-swarm",
            name="Security & Quality Assurance Swarm",
            description="Security analysis, code review, testing, and quality assurance swarm",
            members=[
                "Security Architect",
                "Penetration Tester", 
                "Code Auditor",
                "QA Engineer",
                "Compliance Specialist",
                "Risk Analyst"
            ],
            model_pool="premium"
        ),
        TeamInfo(
            id="research-swarm",
            name="Research & Innovation Swarm",
            description="Research, experimentation, prototyping, and emerging technology swarm", 
            members=[
                "Research Scientist",
                "AI/ML Engineer",
                "Innovation Specialist",
                "Prototype Developer",
                "Technology Scout",
                "Data Scientist"
            ],
            model_pool="premium"
        )
    ]

@app.get("/workflows", response_model=List[WorkflowInfo])
async def get_workflows():
    """Get available workflows."""
    return [
        WorkflowInfo(
            id="pr-lifecycle",
            name="PR Lifecycle",
            description="Complete PR workflow with validation",
            inputs={
                "priority": "high/medium/low",
                "repo": "repository name",
                "branch": "target branch"
            },
            steps=["Plan", "Implement", "Review", "Gates", "Merge"]
        ),
        WorkflowInfo(
            id="code-review",
            name="Code Review",
            description="Automated code review with gates",
            inputs={
                "files": "comma-separated file paths",
                "focus": "security/performance/quality"
            },
            steps=["Analyze", "Critic", "Judge", "Report"]
        )
    ]

# ============================================
# Memory Operations
# ============================================

@app.post("/memory/add")
async def add_memory(request: MemoryRequest):
    """Add entry to Supermemory."""
    if not state.supermemory:
        raise HTTPException(status_code=503, detail="Supermemory not initialized")
    
    entry = MemoryEntry(
        topic=request.topic,
        content=request.content,
        source=request.source,
        tags=request.tags,
        memory_type=MemoryType(request.memory_type)
    )
    
    result = await state.supermemory.add_to_memory(entry)
    return result

@app.post("/memory/search")
async def search_memory(request: SearchRequest):
    """Search Supermemory."""
    if not state.supermemory:
        raise HTTPException(status_code=503, detail="Supermemory not initialized")
    
    entries = await state.supermemory.search_memory(
        query=request.query,
        limit=request.top_k
    )
    
    return {
        "query": request.query,
        "count": len(entries),
        "results": [
            {
                "topic": e.topic,
                "content": e.content,
                "source": e.source,
                "tags": e.tags,
                "type": e.memory_type.value
            }
            for e in entries
        ],
        "entries": [  # Keep for backward compatibility
            {
                "topic": e.topic,
                "content": e.content,
                "source": e.source,
                "tags": e.tags,
                "type": e.memory_type.value
            }
            for e in entries
        ]
    }

# ============================================
# Search & Retrieval
# ============================================

@app.post("/search")
async def hybrid_search(request: SearchRequest):
    """Perform hybrid search with optional GraphRAG."""
    if not state.search_engine:
        raise HTTPException(status_code=503, detail="Search engine not initialized")
    
    # Perform hybrid search
    results = await state.search_engine.search(
        query=request.query,
        top_k=request.top_k,
        use_semantic=request.use_semantic,
        use_bm25=request.use_bm25,
        use_reranker=request.use_reranker
    )
    
    # Add graph context if requested
    graph_context = None
    if request.include_graph and state.graph_rag:
        # Get initial entities from search results
        initial_entities = [r.result.id for r in results[:3]]
        graph_context = state.graph_rag.augment_context_with_graph(
            query=request.query,
            initial_entities=initial_entities,
            max_hops=2
        )
    
    return {
        "query": request.query,
        "results": [
            {
                "id": r.result.id,
                "content": r.result.content[:200],
                "score": r.final_score,
                "citation": r.result.citation,
                "scores": {
                    "semantic": r.semantic_score,
                    "bm25": r.bm25_score,
                    "rerank": r.rerank_score
                }
            }
            for r in results
        ],
        "graph_context": graph_context
    }

# ============================================
# Team & Workflow Execution
# ============================================

async def execute_team_with_gates(
    request: RunRequest
) -> AsyncGenerator[str, None]:
    """Execute AI swarm with full gate evaluation."""
    
    # Start execution
    yield f"data: {json.dumps({'phase': 'initialization', 'token': 'üöÄ Initializing AI Swarm...'})}\n\n"
    await asyncio.sleep(0.3)
    
    # Search for relevant context if memory enabled
    context = []
    if request.use_memory and state.supermemory:
        memories = await state.supermemory.search_memory(request.message, limit=5)
        context = [m.content for m in memories]
        yield f"data: {json.dumps({'phase': 'memory', 'token': f'üß† Retrieved {len(memories)} relevant memories'})}\n\n"
        await asyncio.sleep(0.2)
    
    # Strategic planning phase
    yield f"data: {json.dumps({'phase': 'strategic_planning', 'token': 'üìã Strategic Planner analyzing requirements...'})}\n\n"
    await asyncio.sleep(0.4)
    
    # Architecture design
    yield f"data: {json.dumps({'phase': 'architecture', 'token': 'üèóÔ∏è Lead Architect designing solution...'})}\n\n"
    await asyncio.sleep(0.4)
    
    try:
        # Execute real swarm with the unified team
        yield f"data: {json.dumps({'phase': 'execution', 'token': '‚ö° AI Swarm executing task...'})}\n\n"
        
        # Simulate swarm execution based on team specialization
        team_id = request.team_id or "development-swarm"
        
        # Different execution patterns based on swarm type
        if team_id == "strategic-swarm":
            yield f"data: {json.dumps({'phase': 'analysis', 'token': 'üéØ Chief Architect analyzing business requirements...'})}\n\n"
            await asyncio.sleep(0.4)
            yield f"data: {json.dumps({'phase': 'planning', 'token': 'üìä Strategic Planner creating roadmap...'})}\n\n"
            
        elif team_id == "development-swarm":
            yield f"data: {json.dumps({'phase': 'implementation', 'token': 'üíª Senior Engineers writing code...'})}\n\n"
            await asyncio.sleep(0.4)
            yield f"data: {json.dumps({'phase': 'integration', 'token': 'üîß DevOps Engineer setting up deployment...'})}\n\n"
            
        elif team_id == "security-swarm":
            yield f"data: {json.dumps({'phase': 'audit', 'token': 'üîí Security Architect conducting threat analysis...'})}\n\n"
            await asyncio.sleep(0.4)
            yield f"data: {json.dumps({'phase': 'testing', 'token': 'üîç Penetration Tester running security tests...'})}\n\n"
            
        elif team_id == "research-swarm":
            yield f"data: {json.dumps({'phase': 'research', 'token': 'üî¨ Research Scientist exploring solutions...'})}\n\n"
            await asyncio.sleep(0.4)
            yield f"data: {json.dumps({'phase': 'prototype', 'token': '‚ö° AI/ML Engineer building prototype...'})}\n\n"
        
        await asyncio.sleep(0.5)
        
        # Simulate swarm results
        swarm_results = {
            "team_id": team_id,
            "status": "completed",
            "generator": {"output": f"Solution generated by {team_id}"},
            "critic": {"verdict": "pass", "confidence": 0.85},
            "judge": {"decision": "accept", "rationale": "Solution meets requirements"}
        }
        
        # Stream swarm progress
        if swarm_results.get("generator"):
            yield f"data: {json.dumps({'phase': 'generation', 'token': 'üîß Senior Developers implementing solution...'})}\n\n"
            await asyncio.sleep(0.5)
            
        if swarm_results.get("critic"):
            verdict = swarm_results["critic"].get("verdict", "unknown")
            yield f"data: {json.dumps({'phase': 'review', 'token': f'üîç Code Critic review: {verdict}'})}\n\n"
            await asyncio.sleep(0.3)
            
        if swarm_results.get("judge"):
            decision = swarm_results["judge"].get("decision", "unknown")
            yield f"data: {json.dumps({'phase': 'judgment', 'token': f'‚öñÔ∏è Technical Judge decision: {decision}'})}\n\n"
            await asyncio.sleep(0.3)
        
        # Quality assurance
        yield f"data: {json.dumps({'phase': 'quality', 'token': '‚úÖ Quality Assurance validating output...'})}\n\n"
        await asyncio.sleep(0.3)
        
        # Final results
        yield f"data: {json.dumps({'phase': 'completion', 'swarm_results': swarm_results, 'token': 'üéØ AI Swarm task completed!'})}\n\n"
        
    except Exception as e:
        yield f"data: {json.dumps({'phase': 'error', 'error': str(e), 'token': f'‚ùå Swarm execution error: {str(e)}'})}\n\n"
    
    # Simulate generation
    yield f"data: {json.dumps({'phase': 'generation', 'token': 'üíª Generating solutions...'})}\n\n"
    await asyncio.sleep(1)
    
    # Simulate critic review
    critic_output = {
        "verdict": "pass",
        "findings": {
            "security": [],
            "logic_correctness": [],
            "performance": ["Consider caching for better performance"]
        },
        "must_fix": [],
        "confidence_score": 0.85
    }
    
    yield f"data: {json.dumps({'phase': 'critic', 'token': 'üîç Critic review complete', 'critic': critic_output})}\n\n"
    await asyncio.sleep(0.5)
    
    # Simulate judge decision
    judge_output = {
        "decision": "accept",
        "runner_instructions": [
            "Implement the solution",
            "Add comprehensive tests",
            "Update documentation"
        ],
        "rationale": ["Solution meets requirements", "Code quality is high"],
        "confidence_score": 0.9
    }
    
    yield f"data: {json.dumps({'phase': 'judge', 'token': '‚öñÔ∏è Judge decision: ACCEPT', 'judge': judge_output})}\n\n"
    await asyncio.sleep(0.5)
    
    # Evaluate gates if enabled
    gate_result = {"allowed": True, "status": "ALLOWED"}
    if state.gate_manager and ServerConfig.GATES_ENABLED:
        gate_evaluation = state.gate_manager.evaluate_all(
            critic_output=critic_output,
            judge_output=judge_output,
            acceptance_criteria=["Task completed", "Tests pass"]
        )
        gate_result = {
            "allowed": gate_evaluation["runner_allowed"] or ServerConfig.RUNNER_GATE_OVERRIDE,
            "status": "ALLOWED (DEV MODE)" if ServerConfig.RUNNER_GATE_OVERRIDE else gate_evaluation["overall_status"],
            "scores": {
                "total": gate_evaluation["total_score"],
                "max": gate_evaluation["total_max_score"]
            }
        }
    elif ServerConfig.RUNNER_GATE_OVERRIDE:
        gate_result = {"allowed": True, "status": "ALLOWED (DEV MODE)"}
    
    gate_status = gate_result["status"]
    yield f"data: {json.dumps({'phase': 'gates', 'token': f'üö™ Runner Gate: {gate_status}', 'gates': gate_result})}\n\n"
    
    # Final result
    final_response = {
        "success": True,
        "team_id": request.team_id,
        "message": request.message,
        "critic": critic_output,
        "judge": judge_output,
        "gates": gate_result,
        "tool_calls": [
            {"name": "code_search", "args": {"query": request.message}},
            {"name": "supermemory.search", "args": {"query": request.message}}
        ],
        "citations": [
            "app/main.py:10-25",
            "app/utils.py:45-60"
        ]
    }
    
    yield f"data: {json.dumps({'phase': 'complete', 'final': final_response})}\n\n"
    await asyncio.sleep(0.1)  # Small delay before closing
    yield "data: [DONE]\n\n"

@app.post("/teams/run")
async def run_team(request: RunRequest):
    """Run a team with real swarm execution and streaming response."""
    return StreamingResponse(
        stream_real_swarm_execution(request, state),
        media_type="text/event-stream",
        headers={
            "Cache-Control": "no-cache",
            "Connection": "keep-alive",
            "X-Portkey-Metadata": json.dumps({
                "role": "orchestrator",
                "team": request.team_id,
                "timestamp": datetime.now().isoformat()
            })
        }
    )

@app.post("/workflows/run")
async def run_workflow(request: RunRequest):
    """Run a workflow with streaming response."""
    
    async def execute_workflow():
        steps = ["Preflight", "Planning", "Execution", "Validation", "Completion"]
        
        for i, step in enumerate(steps):
            yield f"data: {json.dumps({'step': i+1, 'name': step, 'token': f'‚ö° {step}...'})}\n\n"
            await asyncio.sleep(0.5)
        
        # Final workflow result
        final = {
            "workflow_id": request.workflow_id,
            "status": "completed",
            "gates": {"accuracy": 8.5, "reliability": True},
            "artifacts": ["output.py", "tests.py", "README.md"]
        }
        
        yield f"data: {json.dumps({'final': final})}\n\n"
        yield "data: [DONE]\n\n"
    
    return StreamingResponse(
        execute_workflow(),
        media_type="text/event-stream"
    )

# ============================================
# Admin & Monitoring
# ============================================

@app.get("/stats")
async def get_stats():
    """Get system statistics."""
    stats = {}
    
    if state.supermemory:
        stats["memory"] = await state.supermemory.get_stats()
    
    if state.embedder:
        stats["embeddings"] = state.embedder.get_stats()
    
    if state.knowledge_graph:
        stats["graph"] = state.knowledge_graph.get_stats()
    
    return stats

@app.post("/index/update")
async def update_index(
    path: str,
    force: bool = False
):
    """Update search index for a path."""
    # This would trigger incremental indexing
    return {
        "status": "indexing",
        "path": path,
        "force": force
    }

# ============================================
# Agno-Compatible Endpoints (Aliases)
# ============================================

@app.get("/agents")
async def get_agents_compat(action: Optional[str] = None):
    """Agno-compatible alias for teams endpoint."""
    # Handle activity polling from UI
    if action == "activity":
        return JSONResponse({
            "agents": [team.model_dump() for team in await get_teams()],
            "activity": {
                "active_tasks": 0,
                "completed_tasks": 0,
                "status": "idle"
            }
        })
    return await get_teams()

@app.get("/api/agents")
async def get_api_agents_compat(action: Optional[str] = None):
    """API-compatible alias for teams endpoint."""
    # Handle activity polling from UI
    if action == "activity":
        return JSONResponse({
            "agents": [team.model_dump() for team in await get_teams()],
            "activity": {
                "active_tasks": 0,
                "completed_tasks": 0,
                "status": "idle"
            }
        })
    return await get_teams()

@app.post("/run/team")
async def run_team_compat(request: RunRequest):
    """Agno-compatible alias for team execution."""
    return await run_team(request)

@app.post("/run/workflow")
async def run_workflow_compat(request: RunRequest):
    """Agno-compatible alias for workflow execution."""
    return await run_workflow(request)

@app.get("/v1/playground/agents", response_model=List[TeamInfo])
async def get_playground_agents():
    """Agno playground-compatible agents endpoint."""
    teams = await get_teams()
    # Transform to agent format if needed
    for team in teams:
        if not hasattr(team, 'agent_id'):
            team.agent_id = team.id
    return teams

@app.get("/v1/playground/teams", response_model=List[TeamInfo])
async def get_playground_teams():
    """Agno playground-compatible teams endpoint."""
    return await get_teams()

@app.post("/v1/playground/agents/{agent_id}/runs")
async def run_playground_agent(agent_id: str, request: RunRequest):
    """Agno playground-compatible agent run endpoint."""
    request.team_id = agent_id
    return await run_team(request)

@app.post("/v1/playground/teams/{team_id}/runs")
async def run_playground_team(team_id: str, request: RunRequest):
    """Agno playground-compatible team run endpoint."""
    request.team_id = team_id
    return await run_team(request)

@app.get("/v1/playground/status")
async def playground_status():
    """Agno playground-compatible status endpoint."""
    return await health_check()

# ============================================
# Main Entry Point
# ============================================

if __name__ == "__main__":
    import uvicorn
    
    # Print configuration
    ServerConfig.print_config()
    
    print(f"""
‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
‚ïë           UNIFIED AGENT API SERVER                    ‚ïë
‚ïë                                                        ‚ïë
‚ïë  Endpoints:                                            ‚ïë
‚ïë  - Health:     http://localhost:{ServerConfig.PORT}/healthz     ‚ïë
‚ïë  - Teams:      http://localhost:{ServerConfig.PORT}/teams       ‚ïë
‚ïë  - Workflows:  http://localhost:{ServerConfig.PORT}/workflows   ‚ïë
‚ïë  - Search:     http://localhost:{ServerConfig.PORT}/search      ‚ïë
‚ïë  - Memory:     http://localhost:{ServerConfig.PORT}/memory/*    ‚ïë
‚ïë                                                        ‚ïë
‚ïë  Features:                                             ‚ïë
‚ïë  ‚úÖ Supermemory MCP                                   ‚ïë
‚ïë  ‚úÖ Dual-tier Embeddings                              ‚ïë
‚ïë  ‚úÖ Hybrid Search (BM25 + Vector)                     ‚ïë
‚ïë  ‚úÖ GraphRAG (Optional)                               ‚ïë
‚ïë  ‚úÖ Evaluation Gates                                  ‚ïë
‚ïë  ‚úÖ Streaming Responses                               ‚ïë
‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù
    """)
    
    uvicorn.run(
        app,
        host=ServerConfig.HOST,
        port=ServerConfig.PORT,
        log_level="info"
    )