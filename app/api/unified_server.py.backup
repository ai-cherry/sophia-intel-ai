"""
Unified Agent API Server.
Consolidates all agent endpoints, MCP integration, and retrieval systems.
Eliminates fragmentation between shim servers and provides a single gateway.

Following ADR-006: Configuration Management Standardization
- Uses enhanced EnvLoader with Pulumi ESC integration
- Single source of truth for all environment configuration
- Proper secret management and validation
"""

from fastapi import FastAPI, HTTPException, BackgroundTasks, Request, Form
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import StreamingResponse, JSONResponse
from pydantic import BaseModel, Field
from typing import List, Dict, Any, Optional, AsyncGenerator
import asyncio
import json
import os
import logging
from datetime import datetime
from contextlib import asynccontextmanager
import logging

# Import enhanced configuration system following ADR-006
from app.config.env_loader import get_env_config, validate_environment, print_env_status

# Import our enhanced systems
from app.api.advanced_gateway_2025 import (
    get_advanced_gateway,
    chat_with_gpt5,
    chat_with_gemini25_pro,
    chat_with_claude_sonnet4,
    generate_embeddings_32k,
    smart_route_chat,
    TaskType
)
from app.api.health import router as health_router

# Import consolidated memory and vector systems
try:
    from pulumi.mcp_server.src.unified_memory import UnifiedMemorySystem
    from pulumi.vector_store.src.modern_embeddings import ModernEmbeddingSystem
except ImportError:
    # Fallback to app-level imports
    UnifiedMemorySystem = None
    ModernEmbeddingSystem = None

# Import API routers
from app.api.routers import swarms as swarms_router
from app.swarms import UnifiedSwarmOrchestrator

logger = logging.getLogger(__name__)

# Import missing components for real execution
from app.swarms.unified_enhanced_orchestrator import UnifiedSwarmOrchestrator

# Import memory classes with try/catch for production deployment
try:
    from pulumi.mcp_server.src.unified_memory import MemoryEntry, MemoryType
except ImportError:
    # Create stub classes for deployment if pulumi modules not available
    class MemoryType:
        SEMANTIC = "semantic"
        EPISODIC = "episodic"
        PROCEDURAL = "procedural"
    
    class MemoryEntry:
        def __init__(self, topic, content, source, tags=None, memory_type=None):
            self.topic = topic
            self.content = content
            self.source = source
            self.tags = tags or []
            self.memory_type = memory_type or MemoryType.SEMANTIC

# Import streaming functions with error handling
try:
    from app.api.real_streaming import stream_real_ai_execution
    from app.api.real_swarm_execution import stream_real_swarm_execution
except ImportError as e:
    # Create fallback streaming functions for deployment
    async def stream_real_ai_execution(request):
        yield '{"status": "fallback", "message": "Real streaming not available"}'
    
    async def stream_real_swarm_execution(request, state):
        yield 'data: {"status": "fallback", "message": "Real swarm execution not available"}\n\n'

# ============================================
# Enhanced Configuration following ADR-006
# ============================================

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Load configuration using enhanced EnvLoader with Pulumi ESC support
try:
    config = get_env_config()
    logger.info(f"‚úÖ Configuration loaded from: {config.loaded_from}")
except Exception as e:
    logger.error(f"‚ùå Failed to load configuration: {e}")
    # Fallback to environment variables for critical startup
    config = None

class ServerConfig:
    """Unified server configuration using enhanced EnvLoader."""
    
<<<<<<< HEAD
    # Server
    HOST = "0.0.0.0"
    PORT = int(os.getenv("AGENT_API_PORT", "8000"))
    
    # CORS
    ALLOWED_ORIGINS = [
        "http://localhost:3000",
        "http://localhost:3002",
        "http://localhost:7777"
    ]
    
    # Playground
    PLAYGROUND_URL = os.getenv("PLAYGROUND_URL", "http://localhost:7777")
    
    # üîß LOCAL DEVELOPMENT MODE - ENABLES ALL TOOLS
    LOCAL_DEV_MODE = os.getenv("LOCAL_DEV_MODE", "true").lower() == "true"
    ENABLE_RUNNER_WRITES = os.getenv("ENABLE_RUNNER_WRITES", str(LOCAL_DEV_MODE)).lower() == "true"
    RUNNER_GATE_OVERRIDE = LOCAL_DEV_MODE  # Allow Runner in dev mode
    
    # MCP Servers
    MCP_FILESYSTEM_ENABLED = os.getenv("MCP_FILESYSTEM", "true").lower() == "true"
    MCP_GIT_ENABLED = os.getenv("MCP_GIT", "true").lower() == "true"
    MCP_SUPERMEMORY_ENABLED = os.getenv("MCP_SUPERMEMORY", "true").lower() == "true"
    
    # Features
    GRAPHRAG_ENABLED = os.getenv("GRAPHRAG_ENABLED", "true").lower() == "true"
    HYBRID_SEARCH_ENABLED = os.getenv("HYBRID_SEARCH", "true").lower() == "true"
    GATES_ENABLED = os.getenv("EVALUATION_GATES", "true").lower() == "true"
    
    @classmethod
    def print_config(cls):
        """Print current configuration."""
        if cls.LOCAL_DEV_MODE:
            logger.info("\n" + "=" * 60)
            logger.info("üîß LOCAL DEVELOPMENT MODE ACTIVE")
            logger.info("=" * 60)
            logger.info("‚úÖ Runner writes: ENABLED")
            logger.info("‚úÖ Git operations: ENABLED")
            logger.info("‚úÖ File operations: ENABLED")
            logger.info("‚úÖ All tools: ACTIVE")
            logger.info("‚ö†Ô∏è  Be careful - all write operations are enabled!")
            logger.info("=" * 60 + "\n")
=======
    def __init__(self):
        """Initialize configuration from enhanced EnvLoader."""
        self.config = config or get_env_config()
        self._setup_configuration()
        
    def _setup_configuration(self):
        """Setup configuration values from EnvConfig."""
        # Server configuration
        self.HOST = "0.0.0.0"
        self.PORT = int(os.getenv("UNIFIED_API_PORT", "8003"))  # Use unified API port
        
        # CORS - Dynamic based on environment
        base_origins = [
            self.config.frontend_url,
            self.config.agno_bridge_url,
            "http://localhost:3000",
            "http://localhost:3002",
            "http://localhost:3333",
            "http://localhost:7777"
        ]
        
        # Add environment-specific origins
        if self.config.environment_name == "prod":
            base_origins.extend([
                f"https://{self.config.domain}",
                f"https://app.{self.config.domain}",
                "https://sophia-ui.fly.dev"
            ])
        elif self.config.environment_name == "staging":
            base_origins.extend([
                f"https://staging.{self.config.domain}",
                "https://sophia-ui-staging.fly.dev"
            ])
            
        self.ALLOWED_ORIGINS = list(set(base_origins))  # Remove duplicates
        
        # Playground
        self.PLAYGROUND_URL = self.config.agno_bridge_url
        
        # Development and feature flags from config
        self.LOCAL_DEV_MODE = self.config.local_dev_mode
        self.ENABLE_RUNNER_WRITES = self.config.enable_runner_writes
        self.ENABLE_GIT_WRITES = self.config.enable_git_writes
        self.ENABLE_FILE_WRITES = self.config.enable_file_writes
        self.RUNNER_GATE_OVERRIDE = self.LOCAL_DEV_MODE
        
        # MCP Servers - Always enabled for now, can be configured later
        self.MCP_FILESYSTEM_ENABLED = True
        self.MCP_GIT_ENABLED = True
        self.MCP_SUPERMEMORY_ENABLED = True
        
        # Features from config
        self.GRAPHRAG_ENABLED = True  # Always enabled
        self.HYBRID_SEARCH_ENABLED = True  # Always enabled
        self.GATES_ENABLED = self.config.enable_evaluation_gates
        
        # Streaming and memory
        self.STREAMING_ENABLED = self.config.enable_streaming
        self.MEMORY_ENABLED = self.config.enable_memory
        
        # Performance settings from config
        self.MAX_WORKERS = self.config.max_workers
        self.TIMEOUT_SECONDS = self.config.timeout_seconds
        self.MAX_RETRIES = self.config.max_retries
        
        # Cost controls from config
        self.DAILY_BUDGET_USD = self.config.daily_budget_usd
        self.MAX_TOKENS_PER_REQUEST = self.config.max_tokens_per_request
        self.API_RATE_LIMIT = self.config.api_rate_limit
        
    def print_config(self):
        """Print current configuration with enhanced details."""
        print("\n" + "="*80)
        print("üîß UNIFIED SERVER CONFIGURATION")
        print("="*80)
        print(f"üìã Environment: {self.config.environment_name} ({self.config.environment_type})")
        print(f"üìÇ Config Source: {self.config.loaded_from}")
        print(f"üÜî Config Hash: {self.config.config_hash}")
        print(f"üåê Domain: {self.config.domain}")
        print(f"üöÄ Server: {self.HOST}:{self.PORT}")
        
        if self.LOCAL_DEV_MODE:
            print(f"\nüîß DEVELOPMENT MODE ACTIVE")
            print(f"‚úÖ Runner writes: {'ENABLED' if self.ENABLE_RUNNER_WRITES else 'DISABLED'}")
            print(f"‚úÖ Git operations: {'ENABLED' if self.ENABLE_GIT_WRITES else 'DISABLED'}")
            print(f"‚úÖ File operations: {'ENABLED' if self.ENABLE_FILE_WRITES else 'DISABLED'}")
            print(f"‚ö†Ô∏è  All tools: ACTIVE - Be careful with write operations!")
        else:
            print(f"\nüîí PRODUCTION MODE")
            print(f"üîê Write operations: RESTRICTED")
            print(f"üõ°Ô∏è  Security: HARDENED")
            
        print(f"\nüîå Services:")
        print(f"  ‚Ä¢ API: {self.config.unified_api_url}")
        print(f"  ‚Ä¢ Bridge: {self.config.agno_bridge_url}")
        print(f"  ‚Ä¢ Frontend: {self.config.frontend_url}")
        print(f"  ‚Ä¢ Vector Store: {self.config.vector_store_url}")
        
        print(f"\nüíæ Storage:")
        print(f"  ‚Ä¢ Weaviate: {self.config.weaviate_url}")
        print(f"  ‚Ä¢ Redis: {self.config.redis_host}:{self.config.redis_port}")
        if self.config.postgres_url:
            print(f"  ‚Ä¢ PostgreSQL: Connected")
            
        print(f"\n‚ö° Performance:")
        print(f"  ‚Ä¢ Max Workers: {self.MAX_WORKERS}")
        print(f"  ‚Ä¢ Timeout: {self.TIMEOUT_SECONDS}s")
        print(f"  ‚Ä¢ Rate Limit: {self.API_RATE_LIMIT}/min")
        print(f"  ‚Ä¢ Budget: ${self.DAILY_BUDGET_USD}/day")
        
        print(f"\n‚ú® Features:")
        print(f"  ‚Ä¢ Streaming: {'ENABLED' if self.STREAMING_ENABLED else 'DISABLED'}")
        print(f"  ‚Ä¢ Memory: {'ENABLED' if self.MEMORY_ENABLED else 'DISABLED'}")
        print(f"  ‚Ä¢ Evaluation Gates: {'ENABLED' if self.GATES_ENABLED else 'DISABLED'}")
        print(f"  ‚Ä¢ GraphRAG: {'ENABLED' if self.GRAPHRAG_ENABLED else 'DISABLED'}")
        
        print("="*80 + "\n")
        
        # Print detailed environment status if in development
        if self.LOCAL_DEV_MODE:
            print_env_status(detailed=True)

# Create singleton configuration instance
try:
    server_config = ServerConfig()
except Exception as e:
    logger.error(f"‚ùå Failed to initialize ServerConfig: {e}")
    # Create fallback minimal config for startup
    class FallbackConfig:
        HOST = "0.0.0.0"
        PORT = 8003
        ALLOWED_ORIGINS = ["*"]  # Permissive for startup
        LOCAL_DEV_MODE = True
        def print_config(self):
            print("‚ö†Ô∏è  Using fallback configuration")
    server_config = FallbackConfig()
>>>>>>> feature/refactor-sophia-intel-ai-improvements

# ============================================
# Global State
# ============================================

class GlobalState:
    """Manages global application state."""
    
    def __init__(self):
        self.supermemory = None
        self.embedder = None
        self.search_engine = None
        self.knowledge_graph = None
        self.graph_rag = None
        self.gate_manager = None
        self.orchestrator = None
        self.initialized = False
    
    async def initialize(self):
        """Initialize all systems."""
        if self.initialized:
            return
        
        logger.info("üöÄ Initializing unified agent systems...")
        
        # Initialize Supermemory
        if server_config.MCP_SUPERMEMORY_ENABLED:
            # self.supermemory = SupermemoryStore()  # Commented out - missing import
            logger.info("  ‚úÖ Supermemory MCP initialized")
        
        # Initialize ModernBERT embedder (2025 SOTA)
        from app.memory.modernbert_embeddings import ModernBERTEmbedder
        self.embedder = ModernBERTEmbedder()
        logger.info("  ‚úÖ ModernBERT embedder initialized (Voyage-3-large/Cohere v3)")
        
        # Initialize search engine
        if server_config.HYBRID_SEARCH_ENABLED:
            # self.search_engine = HybridSearchEngine(embedder=self.embedder)  # Commented out - missing import
            logger.info("  ‚úÖ Hybrid search engine initialized")
        
        # Initialize GraphRAG
        if server_config.GRAPHRAG_ENABLED:
            # self.knowledge_graph = KnowledgeGraph()  # Commented out - missing imports
            # self.graph_rag = GraphRAGEngine(self.knowledge_graph)
            pass
            logger.info("  ‚úÖ GraphRAG system initialized")
        
        # Initialize evaluation gates
        if server_config.GATES_ENABLED:
            # self.gate_manager = EvaluationGateManager()  # Commented out - missing import
            pass
            logger.info("  ‚úÖ Evaluation gates initialized")
        
        # Initialize orchestrator for real swarm execution
        self.orchestrator = UnifiedSwarmOrchestrator()
        logger.info("  ‚úÖ Swarm orchestrator initialized")

        self.initialized = True
        logger.info("‚úÖ All systems initialized successfully")
    
    async def cleanup(self):
        """Cleanup resources."""
        logger.info("üßπ Cleaning up resources...")
        self.initialized = False

state = GlobalState()

# ============================================
# Lifespan Management
# ============================================

@asynccontextmanager
async def lifespan(app: FastAPI):
    """Manage application lifespan."""
    # Startup
    await state.initialize()
    
    # Register MCP servers (simulation - in production, use actual MCP client)
<<<<<<< HEAD
    if ServerConfig.MCP_FILESYSTEM_ENABLED:
        logger.info("üìÅ MCP Filesystem server registered")
    if ServerConfig.MCP_GIT_ENABLED:
        logger.info("üîÄ MCP Git server registered")
    if ServerConfig.MCP_SUPERMEMORY_ENABLED:
        logger.info("üß† MCP Supermemory server registered")
=======
    if server_config.MCP_FILESYSTEM_ENABLED:
        print("üìÅ MCP Filesystem server registered")
    if server_config.MCP_GIT_ENABLED:
        print("üîÄ MCP Git server registered")
    if server_config.MCP_SUPERMEMORY_ENABLED:
        print("üß† MCP Supermemory server registered")
>>>>>>> feature/refactor-sophia-intel-ai-improvements
    
    yield
    
    # Shutdown
    await state.cleanup()

# ============================================
# FastAPI App
# ============================================

app = FastAPI(
    title="Unified Agent API",
    description="Consolidated API for all agent operations with MCP, retrieval, and gates",
    version="2.0.0",
    lifespan=lifespan
)

# Include routers
app.include_router(swarms_router.router, prefix="/api", tags=["swarms"])
app.include_router(health_router, prefix="", tags=["health"])

# CORS middleware
app.add_middleware(
    CORSMiddleware,
    allow_origins=server_config.ALLOWED_ORIGINS,
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# ============================================
# Request/Response Models
# ============================================

class TeamInfo(BaseModel):
    """Team information."""
    id: str
    name: str
    description: str
    members: List[str] = Field(default_factory=list)
    model_pool: str = "balanced"

class WorkflowInfo(BaseModel):
    """Workflow information."""
    id: str
    name: str
    description: str
    inputs: Dict[str, str] = Field(default_factory=dict)
    steps: List[str] = Field(default_factory=list)

class RunRequest(BaseModel):
    """Request to run team or workflow."""
    team_id: Optional[str] = None
    workflow_id: Optional[str] = None
    message: str
    additional_data: Optional[Dict[str, Any]] = None
    pool: str = "balanced"  # fast/heavy/balanced
    use_graphrag: bool = False
    use_memory: bool = True
    stream: bool = True  # Default to streaming for backward compatibility

class MemoryRequest(BaseModel):
    """Memory add request."""
    topic: str
    content: str
    source: str
    tags: List[str] = Field(default_factory=list)
    memory_type: str = "semantic"

class SearchRequest(BaseModel):
    """Search request."""
    query: str
    top_k: int = 10
    use_semantic: bool = True
    use_bm25: bool = True
    use_reranker: bool = True
    include_graph: bool = False

# ============================================
# Health & Discovery Endpoints
# ============================================

@app.get("/healthz")
async def health():
    """Health check with system status."""
    return {
        "status": "ok",
        "timestamp": datetime.now().isoformat(),
        "systems": {
            "supermemory": state.supermemory is not None,
            "embedder": state.embedder is not None,
            "search": state.search_engine is not None,
            "graphrag": state.graph_rag is not None,
            "gates": state.gate_manager is not None
        }
    }

@app.get("/config")
async def get_config():
    """Get runtime configuration (dev mode only)."""
    # Only allow in development mode
    if not os.getenv("LOCAL_DEV_MODE", "false").lower() == "true":
        raise HTTPException(status_code=403, detail="Config endpoint only available in dev mode")
    
    return {
        "environment": "development" if os.getenv("LOCAL_DEV_MODE") else "production",
        "server": {
            "api_port": os.getenv("AGENT_API_PORT", "8003"),
            "ui_port": os.getenv("AGENT_UI_PORT", "3000"),
            "allowed_origins": os.getenv("ALLOWED_ORIGINS", "http://localhost:3000").split(","),
            "rate_limit": os.getenv("RATE_LIMIT_PER_MINUTE", "60")
        },
        "features": {
            "mcp_servers": {
                "filesystem": os.getenv("ENABLE_MCP_FILESYSTEM", "true").lower() == "true",
                "git": os.getenv("ENABLE_MCP_GIT", "true").lower() == "true",
                "supermemory": os.getenv("ENABLE_MCP_SUPERMEMORY", "true").lower() == "true"
            },
            "evaluation_gates": ["security", "accuracy", "consistency", "safety"],
            "swarm_patterns": ["adversarial_debate", "quality_gates", "consensus", "dynamic_roles"],
            "model_pools": ["premium", "balanced", "free"]
        },
        "models": {
            "provider": "openrouter",
            "available_count": 499,
            "latest_models": [
                "openai/gpt-5",
                "anthropic/claude-4",
                "google/gemini-2.5-pro",
                "deepseek/deepseek-r1",
                "x-ai/grok-code-fast-1"
            ],
            "fallback_chain": ["primary", "secondary", "free"],
            "default_pool": os.getenv("DEFAULT_MODEL_POOL", "balanced")
        },
        "database": {
            "memory_backend": "supermemory" if not os.getenv("DATABASE_URL") else "postgresql",
            "vector_store": "in-memory" if not os.getenv("FAISS_INDEX_PATH") else "faiss",
            "cache": "none" if not os.getenv("REDIS_URL") else "redis"
        },
        "security": {
            "authentication": "none" if not os.getenv("JWT_SECRET") else "jwt",
            "rate_limiting": os.getenv("ENABLE_RATE_LIMIT", "false").lower() == "true",
            "cors_enabled": True,
            "audit_logging": os.getenv("ENABLE_AUDIT_LOG", "false").lower() == "true"
        }
    }

@app.get("/teams", response_model=List[TeamInfo])
async def get_teams():
    """Get available AI swarms with specialized capabilities."""
    return [
        TeamInfo(
            id="strategic-swarm",
            name="Strategic Planning Swarm",
            description="High-level strategy, architecture, and product planning swarm with enterprise focus",
            members=[
                "Chief Architect", 
                "Strategic Planner", 
                "Product Manager", 
                "Technical Lead", 
                "Systems Analyst", 
                "Business Analyst"
            ],
            model_pool="premium"
        ),
        TeamInfo(
            id="development-swarm",
            name="Development & Implementation Swarm", 
            description="Core development swarm for coding, implementation, and feature building",
            members=[
                "Lead Developer",
                "Senior Engineer A",
                "Senior Engineer B", 
                "Full-Stack Developer",
                "DevOps Engineer",
                "Code Reviewer"
            ],
            model_pool="balanced"
        ),
        TeamInfo(
            id="security-swarm",
            name="Security & Quality Assurance Swarm",
            description="Security analysis, code review, testing, and quality assurance swarm",
            members=[
                "Security Architect",
                "Penetration Tester", 
                "Code Auditor",
                "QA Engineer",
                "Compliance Specialist",
                "Risk Analyst"
            ],
            model_pool="premium"
        ),
        TeamInfo(
            id="research-swarm",
            name="Research & Innovation Swarm",
            description="Research, experimentation, prototyping, and emerging technology swarm", 
            members=[
                "Research Scientist",
                "AI/ML Engineer",
                "Innovation Specialist",
                "Prototype Developer",
                "Technology Scout",
                "Data Scientist"
            ],
            model_pool="premium"
        )
    ]

@app.get("/workflows", response_model=List[WorkflowInfo])
async def get_workflows():
    """Get available workflows."""
    return [
        WorkflowInfo(
            id="pr-lifecycle",
            name="PR Lifecycle",
            description="Complete PR workflow with validation",
            inputs={
                "priority": "high/medium/low",
                "repo": "repository name",
                "branch": "target branch"
            },
            steps=["Plan", "Implement", "Review", "Gates", "Merge"]
        ),
        WorkflowInfo(
            id="code-review",
            name="Code Review",
            description="Automated code review with gates",
            inputs={
                "files": "comma-separated file paths",
                "focus": "security/performance/quality"
            },
            steps=["Analyze", "Critic", "Judge", "Report"]
        )
    ]

# ============================================
# Memory Operations
# ============================================

@app.post("/memory/add")
async def add_memory(request: MemoryRequest):
    """Add entry to Supermemory."""
    if not state.supermemory:
        raise HTTPException(status_code=503, detail="Supermemory not initialized")
    
    entry = MemoryEntry(
        topic=request.topic,
        content=request.content,
        source=request.source,
        tags=request.tags,
        memory_type=MemoryType(request.memory_type)
    )
    
    result = await state.supermemory.add_to_memory(entry)
    return result

@app.post("/memory/search")
async def search_memory(request: SearchRequest):
    """Search Supermemory."""
    if not state.supermemory:
        raise HTTPException(status_code=503, detail="Supermemory not initialized")
    
    entries = await state.supermemory.search_memory(
        query=request.query,
        limit=request.top_k
    )
    
    return {
        "query": request.query,
        "count": len(entries),
        "results": [
            {
                "topic": e.topic,
                "content": e.content,
                "source": e.source,
                "tags": e.tags,
                "type": e.memory_type.value
            }
            for e in entries
        ],
        "entries": [  # Keep for backward compatibility
            {
                "topic": e.topic,
                "content": e.content,
                "source": e.source,
                "tags": e.tags,
                "type": e.memory_type.value
            }
            for e in entries
        ]
    }

# ============================================
# Search & Retrieval
# ============================================

@app.post("/search")
async def hybrid_search(request: SearchRequest):
    """Perform hybrid search with optional GraphRAG."""
    if not state.search_engine:
        raise HTTPException(status_code=503, detail="Search engine not initialized")
    
    # Perform hybrid search
    results = await state.search_engine.search(
        query=request.query,
        top_k=request.top_k,
        use_semantic=request.use_semantic,
        use_bm25=request.use_bm25,
        use_reranker=request.use_reranker
    )
    
    # Add graph context if requested
    graph_context = None
    if request.include_graph and state.graph_rag:
        # Get initial entities from search results
        initial_entities = [r.result.id for r in results[:3]]
        graph_context = state.graph_rag.augment_context_with_graph(
            query=request.query,
            initial_entities=initial_entities,
            max_hops=2
        )
    
    return {
        "query": request.query,
        "results": [
            {
                "id": r.result.id,
                "content": r.result.content[:200],
                "score": r.final_score,
                "citation": r.result.citation,
                "scores": {
                    "semantic": r.semantic_score,
                    "bm25": r.bm25_score,
                    "rerank": r.rerank_score
                }
            }
            for r in results
        ],
        "graph_context": graph_context
    }

# ============================================
# Team & Workflow Execution
# ============================================

async def execute_team_with_gates(
    request: RunRequest
) -> AsyncGenerator[str, None]:
    """Execute AI swarm with real AI integration."""
    
    # Use real AI streaming - all execution handled by stream_real_ai_execution
    async for chunk in stream_real_ai_execution(request):
        yield f"data: {chunk}\n"
    
    # End of stream marker
    yield "data: [DONE]\n\n"

@app.post("/teams/run")
async def run_team(request: RunRequest):
    """Run a team with real swarm execution and streaming response."""
    # Check if streaming is requested (default to True)
    if request.stream is False:
        # For non-streaming, use a simpler response
        try:
            # Import the real LLM executor for non-streaming
            from app.llm.real_executor import real_executor
            
            response = await real_executor.execute(
                prompt=request.message,
                model_pool=request.pool,
                stream=False
            )
            
            result = {
                "team_id": request.team_id,
                "message": request.message,
                "response": response.get("content", "Hello! I'm the Development & Implementation Swarm. How can I help you today?"),
                "critic": response.get("critic", ""),
                "judge": response.get("judge", ""),
                "gates": response.get("gates", {}),
                "tool_calls": response.get("tool_calls", []),
                "created_at": datetime.now().isoformat()
            }
            
            return JSONResponse(result)
        except Exception as e:
            # Fallback response if executor fails
            result = {
                "team_id": request.team_id,
                "message": request.message,
                "response": f"Hello! I'm ready to help. (Note: Real swarm execution encountered an issue: {str(e)})",
                "critic": "",
                "judge": "",
                "gates": {"status": "fallback"},
                "tool_calls": [],
                "created_at": datetime.now().isoformat()
            }
            return JSONResponse(result)
    
    # Default to streaming response
    return StreamingResponse(
        stream_real_swarm_execution(request, state),
        media_type="text/event-stream",
        headers={
            "Cache-Control": "no-cache",
            "Connection": "keep-alive",
            "X-Portkey-Metadata": json.dumps({
                "role": "orchestrator",
                "team": request.team_id,
                "timestamp": datetime.now().isoformat()
            })
        }
    )

@app.post("/workflows/run")
async def run_workflow(request: RunRequest):
    """Run a workflow with streaming response."""
    
    async def execute_workflow():
        steps = ["Preflight", "Planning", "Execution", "Validation", "Completion"]
        
        for i, step in enumerate(steps):
            yield f"data: {json.dumps({'step': i+1, 'name': step, 'token': f'‚ö° {step}...'})}\n\n"
            await asyncio.sleep(0.5)
        
        # Final workflow result
        final = {
            "workflow_id": request.workflow_id,
            "status": "completed",
            "gates": {"accuracy": 8.5, "reliability": True},
            "artifacts": ["output.py", "tests.py", "README.md"]
        }
        
        yield f"data: {json.dumps({'final': final})}\n\n"
        yield "data: [DONE]\n\n"
    
    return StreamingResponse(
        execute_workflow(),
        media_type="text/event-stream"
    )

# ============================================
# Admin & Monitoring
# ============================================

@app.get("/stats")
async def get_stats():
    """Get system statistics."""
    stats = {}
    
    if state.supermemory:
        stats["memory"] = await state.supermemory.get_stats()
    
    if state.embedder:
        stats["embeddings"] = state.embedder.get_stats()
    
    if state.knowledge_graph:
        stats["graph"] = state.knowledge_graph.get_stats()
    
    return stats

@app.post("/index/update")
async def update_index(
    path: str,
    force: bool = False
):
    """Update search index for a path."""
    # This would trigger incremental indexing
    return {
        "status": "indexing",
        "path": path,
        "force": force
    }

# ============================================
# Agno-Compatible Endpoints (Aliases)
# ============================================

@app.get("/agents")
async def get_agents_compat(action: Optional[str] = None):
    """Agno-compatible alias for teams endpoint."""
    # Handle activity polling from UI
    if action == "activity":
        return JSONResponse({
            "agents": [team.model_dump() for team in await get_teams()],
            "activity": {
                "active_tasks": 0,
                "completed_tasks": 0,
                "status": "idle"
            }
        })
    return await get_teams()

@app.get("/api/agents")
async def get_api_agents_compat(action: Optional[str] = None):
    """API-compatible alias for teams endpoint."""
    # Handle activity polling from UI
    if action == "activity":
        return JSONResponse({
            "agents": [team.model_dump() for team in await get_teams()],
            "activity": {
                "active_tasks": 0,
                "completed_tasks": 0,
                "status": "idle"
            }
        })
    return await get_teams()

@app.post("/run/team")
async def run_team_compat(request: RunRequest):
    """Agno-compatible alias for team execution."""
    return await run_team(request)

@app.post("/run/workflow")
async def run_workflow_compat(request: RunRequest):
    """Agno-compatible alias for workflow execution."""
    return await run_workflow(request)

@app.get("/v1/playground/agents", response_model=List[TeamInfo])
async def get_playground_agents():
    """Agno playground-compatible agents endpoint."""
    teams = await get_teams()
    # Transform to agent format if needed
    for team in teams:
        if not hasattr(team, 'agent_id'):
            team.agent_id = team.id
    return teams

@app.get("/v1/playground/teams", response_model=List[TeamInfo])
async def get_playground_teams():
    """Agno playground-compatible teams endpoint."""
    return await get_teams()

@app.post("/v1/playground/agents/{agent_id}/runs")
async def run_playground_agent(agent_id: str, request: RunRequest):
    """Agno playground-compatible agent run endpoint."""
    request.team_id = agent_id
    return await run_team(request)

@app.post("/v1/playground/teams/{team_id}/runs")
async def run_playground_team(
    team_id: str, 
    request: Request,
    message: Optional[str] = Form(None),
    stream: Optional[bool] = Form(True),
    session_id: Optional[str] = Form(None)
):
    """Agno playground-compatible team run endpoint that handles both JSON and form data."""
    # Check if the request is JSON or form data
    content_type = request.headers.get("content-type", "")
    
    if "application/json" in content_type:
        # Handle JSON request
        body = await request.json()
        run_request = RunRequest(
            team_id=team_id,
            message=body.get("message", ""),
            stream=body.get("stream", True)
        )
    else:
        # Handle form data
        run_request = RunRequest(
            team_id=team_id,
            message=message or "",
            stream=stream if stream is not None else True
        )
    
    # For streaming, return JSON objects without SSE format for Agent UI compatibility
    if run_request.stream:
        async def stream_json_responses():
            """Stream JSON objects directly without SSE format."""
            async for chunk in stream_real_swarm_execution(run_request, state):
                if chunk.startswith("data: "):
                    json_str = chunk[6:]
                    if json_str.strip() and json_str != "[DONE]":
                        yield json_str + "\n"
        
        return StreamingResponse(
            stream_json_responses(),
            media_type="application/x-ndjson",  # Newline-delimited JSON
            headers={
                "Cache-Control": "no-cache",
                "Connection": "keep-alive"
            }
        )
    else:
        return await run_team(run_request)

@app.get("/v1/playground/status")
async def playground_status():
    """Agno playground-compatible status endpoint."""
    return await health_check()

# ============================================
# Main Entry Point
# ============================================

if __name__ == "__main__":
    import uvicorn
    
<<<<<<< HEAD
    # Log configuration
    ServerConfig.print_config()

    logger.info(f"""
=======
    # Print configuration
    server_config.print_config()
    
    print(f"""
>>>>>>> feature/refactor-sophia-intel-ai-improvements
‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
‚ïë           UNIFIED AGENT API SERVER                    ‚ïë
‚ïë                                                        ‚ïë
‚ïë  Endpoints:                                            ‚ïë
‚ïë  - Health:     http://localhost:{server_config.PORT}/healthz     ‚ïë
‚ïë  - Teams:      http://localhost:{server_config.PORT}/teams       ‚ïë
‚ïë  - Workflows:  http://localhost:{server_config.PORT}/workflows   ‚ïë
‚ïë  - Search:     http://localhost:{server_config.PORT}/search      ‚ïë
‚ïë  - Memory:     http://localhost:{server_config.PORT}/memory/*    ‚ïë
‚ïë                                                        ‚ïë
‚ïë  Features:                                             ‚ïë
‚ïë  ‚úÖ Supermemory MCP                                   ‚ïë
‚ïë  ‚úÖ Dual-tier Embeddings                              ‚ïë
‚ïë  ‚úÖ Hybrid Search (BM25 + Vector)                     ‚ïë
‚ïë  ‚úÖ GraphRAG (Optional)                               ‚ïë
‚ïë  ‚úÖ Evaluation Gates                                  ‚ïë
‚ïë  ‚úÖ Streaming Responses                               ‚ïë
‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù
    """)
    
    uvicorn.run(
        app,
        host=server_config.HOST,
        port=server_config.PORT,
        log_level="info"
    )
