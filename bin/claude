#!/usr/bin/env bash
# Claude CLI shim
# Routes to server proxy (Portkey VK) using centralized aliases.

set -euo pipefail

ROOT_DIR="$( cd "$( dirname "${BASH_SOURCE[0]}" )"/.. && pwd )"
LITELLM_PROXY_URL="${LITELLM_PROXY_URL:-http://localhost:4000}"

usage() {
  cat <<EOF
claude: simple Claude chat wrapper (LiteLLM-backed)

Usage:
  claude -p "prompt" [--model <name>]
  claude                    # interactive via litellm-cli

Options:
  --model <name>  Override model (defaults via config/models.json alias 'claude')

Notes:
  - Requires LiteLLM proxy on :4000 (use: bin/litellm-cli start-proxy)
  - Uses config/models.json for alias resolution via bin/llm-models
EOF
}

MODEL=""
PROMPT=""
ARGS=()
while (( "$#" )); do
  case "$1" in
    -p|--prompt)
      shift; PROMPT="${1:-}"; shift || true;;
    --model)
      shift; MODEL="${1:-}"; shift || true; ;;
    -h|--help|help)
      usage; exit 0; ;;
    *) ARGS+=("$1"); shift || true; ;;
  esac
done

# Resolve default model if not provided
if [ -z "$MODEL" ]; then
  if [ -x "$ROOT_DIR/bin/llm-models" ]; then
    MODEL=$("$ROOT_DIR/bin/llm-models" resolve cli claude 2>/dev/null || true)
  fi
  MODEL=${MODEL:-claude-3-5-sonnet}
fi

# One-shot prompt path via LiteLLM proxy
if [ -n "$PROMPT" ]; then
  # Use local server proxy (Studio API)
  RESP=$(curl -s -X POST "http://localhost:3000/api/chat" \
    -H 'Content-Type: application/json' \
    -d "{\"model\": \"$MODEL\", \"messages\": [{\"role\": \"user\", \"content\": $(printf %s "$PROMPT" | python3 -c 'import json,sys; print(json.dumps(sys.stdin.read().strip()))')}]}")
  # Extract message content (python fallback for portability)
  python3 - "$RESP" << 'PY'
import json,sys
data=json.loads(sys.argv[1])
try:
    print(data['choices'][0]['message']['content'])
except Exception as e:
    print(json.dumps(data,indent=2))
    sys.exit(1)
PY
  exit $?
fi

echo "Interactive mode not supported in this shim. Use Studio UI or call /api/chat." >&2
usage
exit 127
