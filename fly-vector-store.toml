# Fly.io Configuration for Sophia Vector Store
app = "sophia-vector"
primary_region = "sjc"  # San Jose - closest to California
kill_signal = "SIGINT"
kill_timeout = "5s"

[build]
  dockerfile = "./pulumi/vector-store/Dockerfile"

[env]
  VECTOR_STORE_PORT = "8005"
  PYTHONPATH = "/app"
  PYTHONUNBUFFERED = "1"
  LOCAL_DEV_MODE = "false"
  
  # Internal service URLs (will use Fly.io internal networking)
  WEAVIATE_URL = "http://sophia-weaviate.internal:8080"
  
  # Modern 3-tier embeddings configuration
  EMBEDDING_TIER_S_MODEL = "voyage-3-large"
  EMBEDDING_TIER_A_MODEL = "cohere/embed-multilingual-v3.0"
  EMBEDDING_TIER_B_MODEL = "BAAI/bge-base-en-v1.5"
  
  # Gateway configuration
  PORTKEY_BASE_URL = "https://api.portkey.ai/v1"
  
  # Feature flags
  USE_REAL_APIS = "true"
  ENABLE_API_VALIDATION = "true"
  ENABLE_EMBEDDING_CACHE = "true"

[experimental]
  auto_rollback = true
  enable_consul = true

[services]
  protocol = "tcp"
  internal_port = 8005
  auto_stop_machines = true
  auto_start_machines = true
  min_machines_running = 1

  [[services.ports]]
    port = 80
    handlers = ["http"]
    force_https = true

  [[services.ports]]
    port = 443
    handlers = ["tls", "http"]
  
  [services.concurrency]
    type = "connections"
    hard_limit = 200
    soft_limit = 150

  [[services.tcp_checks]]
    interval = "15s"
    timeout = "3s"
    grace_period = "1s"
    restart_limit = 6

  [[services.http_checks]]
    interval = "30s"
    grace_period = "5s"
    method = "GET"
    path = "/health"
    protocol = "http"
    timeout = "3s"
    tls_skip_verify = false
    
    [services.http_checks.headers]
      X-Health-Check = "fly.io"

# Persistent storage for embedding cache
[mounts]
  source = "vector_cache"
  destination = "/cache"
  initial_size = "10gb"

# Metrics endpoint for monitoring
[metrics]
  port = 9091
  path = "/metrics"

# Machine configuration optimized for embedding processing
[[vm]]
  cpu_kind = "shared"
  cpus = 2
  memory_mb = 2048
  
# Auto-scaling configuration - more aggressive for embedding workloads
[scaling]
  min_machines_running = 1
  max_machines_running = 12
  
  [[scaling.metrics]]
    type = "cpu"
    target = 65  # Lower threshold for embedding processing
  
  [[scaling.metrics]]
    type = "memory"
    target = 70
    
  [[scaling.metrics]]
    type = "requests"
    target = 80  # Scale earlier for embedding requests