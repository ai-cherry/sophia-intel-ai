[tool:pytest]
# Pytest Configuration File

# Test Discovery
testpaths = tests
python_files = test_*.py
python_classes = Test*
python_functions = test_*

# Test Execution
addopts =
    # Verbosity and output
    -v
    --tb=short
    --strict-markers
    --color=yes

    # Coverage settings
    --cov=app
    --cov-report=term-missing
    --cov-report=html:htmlcov
    --cov-report=xml:coverage.xml
    --cov-fail-under=80

    # Parallel execution
    -n auto
    --dist=loadfile

    # Show slowest tests
    --durations=10

    # Capture settings
    --capture=no
    -s

    # Warnings
    --disable-warnings
    -W ignore::DeprecationWarning

    # Test order
    --random-order

    # Async support
    --asyncio-mode=auto

# Markers
markers =
    # Test categories
    unit: Unit tests
    integration: Integration tests
    behavior: Behavior tests
    e2e: End-to-end tests
    load: Load/performance tests

    # Domain-specific
    artemis: Artemis domain tests
    sophia: Sophia domain tests
    shared: Shared services tests

    # Component-specific
    factory: Factory pattern tests
    orchestrator: Orchestrator tests
    mcp: MCP integration tests
    resilience: Resilience pattern tests
    swarm: Swarm behavior tests

    # Special markers
    slow: Slow running tests (> 5 seconds)
    skip_ci: Skip in CI/CD pipeline
    requires_network: Requires network connectivity
    requires_db: Requires database connection
    flaky: Known flaky tests
    smoke: Smoke tests for quick validation
    critical: Critical path tests

    # Performance markers
    benchmark: Performance benchmark tests
    stress: Stress tests
    spike: Spike tests
    soak: Soak tests

# Coverage Configuration
[coverage:run]
source = app
omit =
    */tests/*
    */test_*
    */__pycache__/*
    */venv/*
    */.venv/*
    */migrations/*
    */config/*
    */static/*
    */templates/*

parallel = true
concurrency = multiprocessing

[coverage:report]
precision = 2
show_missing = true
skip_covered = false
skip_empty = true

exclude_lines =
    # Standard excludes
    pragma: no cover
    def __repr__
    def __str__
    raise AssertionError
    raise NotImplementedError
    if __name__ == .__main__.:
    if TYPE_CHECKING:
    @abstractmethod
    @abc.abstractmethod

    # Debugging
    if self.debug:
    if settings.DEBUG

    # Defensive programming
    except ImportError
    except Exception as e:

[coverage:html]
directory = htmlcov
title = Sophia-Artemis Test Coverage Report

[coverage:xml]
output = coverage.xml

# Logging Configuration
[tool:pytest:logging]
log_cli = true
log_cli_level = INFO
log_cli_format = %(asctime)s [%(levelname)8s] %(name)s - %(message)s
log_cli_date_format = %Y-%m-%d %H:%M:%S

log_file = tests/logs/pytest.log
log_file_level = DEBUG
log_file_format = %(asctime)s [%(levelname)8s] [%(filename)s:%(lineno)d] %(name)s - %(message)s
log_file_date_format = %Y-%m-%d %H:%M:%S

# Timeout Configuration
[tool:pytest:timeout]
timeout = 300  # 5 minutes default timeout
timeout_method = thread
timeout_func_only = false

# Flaky Test Configuration
[tool:pytest:flaky]
max_runs = 3
min_passes = 1

# XDist Configuration (Parallel Testing)
[tool:pytest:xdist]
looponfail = false
numprocesses = auto

# Environment Variables for Tests
[tool:pytest:env]
TEST_ENV = true
LOG_LEVEL = DEBUG
DOMAIN_ENFORCER_ENABLED = true
MCP_MOCK_ENABLED = true
RESILIENCE_ENABLED = true
MAX_CONCURRENT_TASKS = 8

# Test Ordering
[tool:pytest:ordering]
# Run tests in this order:
# 1. Unit tests (fastest)
# 2. Integration tests
# 3. Behavior tests
# 4. E2E tests
# 5. Load tests (slowest)

# Benchmark Configuration
[tool:pytest:benchmark]
only = false
skip = false
disable_gc = true
min_rounds = 5
min_time = 0.000005
max_time = 1.0
calibration_precision = 10
warmup = true
warmup_iterations = 100000

# Mock Configuration
[tool:pytest:mock]
mock_use_standalone_module = true

# Asyncio Configuration
[tool:pytest:asyncio]
asyncio_mode = auto
asyncio_default_fixture_loop_scope = function

# Hypothesis Configuration (Property-based Testing)
[tool:pytest:hypothesis]
max_examples = 100
deadline = 5000  # milliseconds
suppress_health_check = [too_slow]
verbosity = normal

# Doctest Configuration
[tool:pytest:doctest]
doctest_optionflags = NORMALIZE_WHITESPACE IGNORE_EXCEPTION_DETAIL
doctest_encoding = utf-8

# HTML Report Configuration
[tool:pytest:html]
self_contained_html = true
css = tests/assets/pytest_report.css
title = Sophia-Artemis Test Report

# JSON Report Configuration
[tool:pytest:json]
json_report = test_report.json
json_report_indent = 4
json_report_omit = log

# XML Report Configuration
[tool:pytest:xml]
junit_suite_name = Sophia-Artemis Test Suite
junit_logging = all
junit_log_passing_tests = true
junit_duration_report = total
junit_family = xunit2

# Test Requirements
[tool:pytest:requirements]
min_python_version = 3.9
required_plugins =
    pytest-asyncio
    pytest-cov
    pytest-xdist
    pytest-timeout
    pytest-mock
    pytest-benchmark
    pytest-html
    pytest-json-report
    pytest-random-order
    pytest-flaky
    pytest-env
