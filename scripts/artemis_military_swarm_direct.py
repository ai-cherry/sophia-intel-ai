#!/usr/bin/env python3
"""
ARTEMIS MILITARY SWARM DIRECT - YOUR EXACT MODELS, NO BULLSHIT
Running 4 agents simultaneously with the exact providers you specified
"""

import asyncio
import json
import os
import sys
import time
from concurrent.futures import ThreadPoolExecutor
from datetime import datetime
from typing import Any, Dict, List

import httpx

sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from app.core.aimlapi_config import aimlapi_manager
from app.core.enhanced_llm_router import LLMProviderType, enhanced_router

# Try to import Google Gemini if available
try:
    import google.generativeai as genai

    GEMINI_AVAILABLE = True
except ImportError:
    GEMINI_AVAILABLE = False
    print("‚ö†Ô∏è Google Gemini not installed, will use fallback")


class ArtemisMilitarySwarmDirect:
    """Direct military swarm implementation - no broken dependencies"""

    def __init__(self):
        self.executor = ThreadPoolExecutor(max_workers=4)
        self.results = {}

        # COMPREHENSIVE REPOSITORY SCAN PROMPT - IDENTICAL FOR ALL AGENTS
        self.universal_prompt = {
            "system": """You are a tactical military intelligence agent in the Artemis swarm conducting comprehensive repository analysis.

MISSION: Complete repository reconnaissance of sophia-intel-ai

TACTICAL SCAN PRIORITIES:

1. CODE REDUNDANCIES & ARCHITECTURAL ISSUES (CRITICAL)
   - Duplicate functions/methods (expecting 300+ instances)
   - Repeated configuration blocks (logger declarations, imports)
   - Architectural redundancies between /app/artemis/ and /app/sophia/
   - Factory pattern duplications (6+ similar implementations)
   - Base orchestrator class overlaps

2. SECURITY VULNERABILITIES (CRITICAL)
   - API key exposure in /app/core/portkey_config.py (14 virtual keys)
   - AIMLAPI configuration security gaps
   - Hardcoded secrets or credentials
   - WebSocket authentication missing
   - CORS misconfigurations
   - Unprotected endpoints

3. MEMORY & EMBEDDING ARCHITECTURE (HIGH)
   - Memory systems efficiency in /app/memory/
   - Vector database integrations (Pinecone, Weaviate, Qdrant)
   - Embedding strategy effectiveness
   - Chunking approaches
   - RAG implementation quality
   - Context window management

4. PERFORMANCE BOTTLENECKS (HIGH)
   - Database connection pooling
   - Query optimization opportunities
   - Caching strategy gaps
   - Memory leaks
   - Inefficient algorithms
   - Scalability constraints

REPORTING REQUIREMENTS:
- Provide SPECIFIC file paths and line numbers
- Include code snippets for critical issues
- Prioritize findings: CRITICAL > HIGH > MEDIUM > LOW
- Give actionable tactical recommendations
- Estimate effort/impact for each fix

Focus on the most impactful issues that affect security, performance, and maintainability.""",
            "user": """Execute comprehensive analysis of the sophia-intel-ai repository at /Users/lynnmusil/sophia-intel-ai.

Scan ALL directories but focus on:
- /app/core/ (configuration and security)
- /app/artemis/ and /app/sophia/ (compare for duplication)
- /app/memory/ (data architecture)
- /app/orchestrators/ (base class redundancies)
- /app/factories/ (pattern duplications)

Report findings with military precision. Include specific remediation steps.""",
        }

    def _run_agent1_gemini_direct(self) -> Dict[str, Any]:
        """Agent 1: RECON - Gemini 2.5 Flash via Google Direct API"""
        start_time = time.time()
        agent_name = "RECON-ALPHA"
        model = "gemini-2.5-flash"

        print(f"\nüéØ [T+0.0s] DEPLOYING {agent_name} (Gemini 2.5 Flash)")

        try:
            google_key = os.getenv("GOOGLE_API_KEY", "")

            if google_key and GEMINI_AVAILABLE:
                # Use Google Gemini directly
                genai.configure(api_key=google_key)
                model_gemini = genai.GenerativeModel("gemini-2.5-flash")

                # Combine prompts for Gemini
                full_prompt = (
                    f"{self.universal_prompt['system']}\n\n{self.universal_prompt['user']}"
                )

                response = model_gemini.generate_content(
                    full_prompt,
                    generation_config=genai.types.GenerationConfig(
                        temperature=0.3, max_output_tokens=8000
                    ),
                )

                elapsed = time.time() - start_time
                content = response.text if response else "No response"

                print(f"‚úÖ [T+{elapsed:.1f}s] {agent_name}: Intel gathered via Google Direct")

                return {
                    "agent_number": 1,
                    "agent_name": agent_name,
                    "military_role": "RECONNAISSANCE",
                    "model": model,
                    "provider": "Google Gemini Direct API",
                    "execution_time": elapsed,
                    "findings": content,
                    "response_length": len(content),
                    "timestamp": datetime.now().isoformat(),
                }
            else:
                # Fallback to Portkey
                messages = [
                    {"role": "system", "content": self.universal_prompt["system"]},
                    {"role": "user", "content": self.universal_prompt["user"]},
                ]

                response = enhanced_router.create_completion(
                    messages=messages,
                    model="gemini-2.5-flash",
                    provider_type=LLMProviderType.PORTKEY,
                    temperature=0.3,
                    max_tokens=8000,
                )

                elapsed = time.time() - start_time
                content = (
                    response.choices[0].message.content
                    if hasattr(response, "choices")
                    else str(response)
                )

                print(f"‚úÖ [T+{elapsed:.1f}s] {agent_name}: Intel gathered via Portkey")

                return {
                    "agent_number": 1,
                    "agent_name": agent_name,
                    "military_role": "RECONNAISSANCE",
                    "model": model,
                    "provider": "Portkey (Fallback)",
                    "execution_time": elapsed,
                    "findings": content,
                    "response_length": len(content),
                    "timestamp": datetime.now().isoformat(),
                }

        except Exception as e:
            elapsed = time.time() - start_time
            print(f"‚ùå [T+{elapsed:.1f}s] {agent_name}: Failed - {str(e)[:100]}")
            return {
                "agent_number": 1,
                "agent_name": agent_name,
                "military_role": "RECONNAISSANCE",
                "execution_time": elapsed,
                "error": str(e),
                "timestamp": datetime.now().isoformat(),
            }

    def _run_agent2_grok_alternative(self) -> Dict[str, Any]:
        """Agent 2: STRIKE - Using alternative fast model since no OpenRouter key"""
        start_time = time.time()
        agent_name = "STRIKE-BRAVO"

        # Since no OpenRouter, use a fast alternative
        model = "gpt-4o-mini"  # Fast alternative

        print(f"\n‚ö° [T+0.0s] DEPLOYING {agent_name} (Fast Alternative)")

        messages = [
            {"role": "system", "content": self.universal_prompt["system"]},
            {"role": "user", "content": self.universal_prompt["user"]},
        ]

        try:
            response = enhanced_router.create_completion(
                messages=messages,
                model=model,
                provider_type=LLMProviderType.PORTKEY,
                temperature=0.3,
                max_tokens=8000,
            )

            elapsed = time.time() - start_time
            content = (
                response.choices[0].message.content
                if hasattr(response, "choices")
                else str(response)
            )

            print(f"‚úÖ [T+{elapsed:.1f}s] {agent_name}: Strike complete")

            return {
                "agent_number": 2,
                "agent_name": agent_name,
                "military_role": "STRIKE TEAM",
                "model": model,
                "provider": "Portkey (Alternative to Grok)",
                "execution_time": elapsed,
                "findings": content,
                "response_length": len(content),
                "timestamp": datetime.now().isoformat(),
            }

        except Exception as e:
            elapsed = time.time() - start_time
            print(f"‚ùå [T+{elapsed:.1f}s] {agent_name}: Failed - {str(e)[:100]}")
            return {
                "agent_number": 2,
                "agent_name": agent_name,
                "military_role": "STRIKE TEAM",
                "execution_time": elapsed,
                "error": str(e),
                "timestamp": datetime.now().isoformat(),
            }

    def _run_agent3_glm(self) -> Dict[str, Any]:
        """Agent 3: INTEL - GLM-4.5-Air via AIMLAPI"""
        start_time = time.time()
        agent_name = "INTEL-CHARLIE"
        model = "zhipu/glm-4.5-air"

        print(f"\nüîç [T+0.0s] DEPLOYING {agent_name} (GLM-4.5-Air)")

        messages = [
            {"role": "system", "content": self.universal_prompt["system"]},
            {"role": "user", "content": self.universal_prompt["user"]},
        ]

        try:
            response = aimlapi_manager.chat_completion(
                model=model, messages=messages, temperature=0.3, max_tokens=8000
            )

            elapsed = time.time() - start_time

            # Extract content
            if hasattr(response, "choices"):
                content = response.choices[0].message.content
            elif isinstance(response, dict) and "choices" in response:
                content = response["choices"][0]["message"]["content"]
            else:
                content = str(response) if response else "No response"

            print(f"‚úÖ [T+{elapsed:.1f}s] {agent_name}: Intelligence processed")

            return {
                "agent_number": 3,
                "agent_name": agent_name,
                "military_role": "INTELLIGENCE",
                "model": model,
                "provider": "AIMLAPI",
                "execution_time": elapsed,
                "findings": content,
                "response_length": len(content) if content else 0,
                "timestamp": datetime.now().isoformat(),
            }

        except Exception as e:
            elapsed = time.time() - start_time
            print(f"‚ùå [T+{elapsed:.1f}s] {agent_name}: Failed - {str(e)[:100]}")
            return {
                "agent_number": 3,
                "agent_name": agent_name,
                "military_role": "INTELLIGENCE",
                "execution_time": elapsed,
                "error": str(e),
                "timestamp": datetime.now().isoformat(),
            }

    def _run_agent4_llama(self) -> Dict[str, Any]:
        """Agent 4: SCOUT - Llama-4-Scout via AIMLAPI"""
        start_time = time.time()
        agent_name = "SCOUT-DELTA"
        model = "meta-llama/llama-4-scout"

        print(f"\nüéØ [T+0.0s] DEPLOYING {agent_name} (Llama-4-Scout)")

        messages = [
            {"role": "system", "content": self.universal_prompt["system"]},
            {"role": "user", "content": self.universal_prompt["user"]},
        ]

        try:
            response = aimlapi_manager.chat_completion(
                model=model, messages=messages, temperature=0.3, max_tokens=8000
            )

            elapsed = time.time() - start_time

            # Extract content
            if hasattr(response, "choices"):
                content = response.choices[0].message.content
            elif isinstance(response, dict) and "choices" in response:
                content = response["choices"][0]["message"]["content"]
            else:
                content = str(response) if response else "No response"

            print(f"‚úÖ [T+{elapsed:.1f}s] {agent_name}: Reconnaissance complete")

            return {
                "agent_number": 4,
                "agent_name": agent_name,
                "military_role": "SCOUT",
                "model": model,
                "provider": "AIMLAPI",
                "execution_time": elapsed,
                "findings": content,
                "response_length": len(content) if content else 0,
                "timestamp": datetime.now().isoformat(),
            }

        except Exception as e:
            elapsed = time.time() - start_time
            print(f"‚ùå [T+{elapsed:.1f}s] {agent_name}: Failed - {str(e)[:100]}")
            return {
                "agent_number": 4,
                "agent_name": agent_name,
                "military_role": "SCOUT",
                "execution_time": elapsed,
                "error": str(e),
                "timestamp": datetime.now().isoformat(),
            }

    async def deploy_swarm(self) -> Dict[str, Any]:
        """Deploy all 4 agents SIMULTANEOUSLY"""
        print("\n" + "=" * 70)
        print(" ARTEMIS MILITARY SWARM - TACTICAL DEPLOYMENT")
        print("=" * 70)
        print("\nüìã MISSION: Repository Tactical Analysis")
        print("üìç TARGET: /Users/lynnmusil/sophia-intel-ai")
        print("\n‚öîÔ∏è DEPLOYING UNITS:")
        print("  ‚Ä¢ RECON-ALPHA: Gemini 2.5 Flash (Google Direct)")
        print("  ‚Ä¢ STRIKE-BRAVO: Fast Alternative (No OpenRouter)")
        print("  ‚Ä¢ INTEL-CHARLIE: GLM-4.5-Air (AIMLAPI)")
        print("  ‚Ä¢ SCOUT-DELTA: Llama-4-Scout (AIMLAPI)\n")

        start_time = time.time()

        print("üöÅ ALL UNITS DEPLOYING SIMULTANEOUSLY...")

        loop = asyncio.get_event_loop()

        # Launch ALL agents in PARALLEL
        tasks = [
            loop.run_in_executor(self.executor, self._run_agent1_gemini_direct),
            loop.run_in_executor(self.executor, self._run_agent2_grok_alternative),
            loop.run_in_executor(self.executor, self._run_agent3_glm),
            loop.run_in_executor(self.executor, self._run_agent4_llama),
        ]

        # Wait for all to complete
        results = await asyncio.gather(*tasks)

        total_time = time.time() - start_time

        # Compile results
        self.results = {
            "timestamp": datetime.now().isoformat(),
            "swarm_type": "Artemis Military Swarm Direct",
            "execution_mode": "SIMULTANEOUS DEPLOYMENT",
            "total_execution_time": total_time,
            "agent_count": 4,
            "agents": results,
        }

        # Save results
        output_file = f"artemis_military_direct_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        with open(output_file, "w") as f:
            json.dump(self.results, f, indent=2)

        print(f"\n‚è±Ô∏è  Mission Time: {total_time:.1f} seconds")
        print(f"üíæ Intel saved to {output_file}")

        return self.results

    def score_agents(self, results: Dict[str, Any]) -> Dict[str, Any]:
        """Score each agent's performance"""
        scoring = {
            "mission": "Agent Performance Evaluation",
            "timestamp": datetime.now().isoformat(),
            "agents": [],
        }

        for agent in results["agents"]:
            if "error" not in agent:
                findings = agent.get("findings", "")

                scores = {
                    "quality": min(10, len(findings) / 1000),  # Quality based on depth
                    "speed": max(0, 10 - agent.get("execution_time", 0) / 3),  # Speed score
                    "accuracy": 5,  # Base accuracy (would need manual verification)
                    "thoroughness": min(10, findings.count("\n") / 10),  # Based on structure
                    "prioritization": (
                        5
                        if any(
                            word in findings.upper()
                            for word in ["CRITICAL", "HIGH", "MEDIUM", "LOW"]
                        )
                        else 3
                    ),
                    "communication": 5 if any(char in findings for char in ["‚Ä¢", "-", "#"]) else 3,
                }

                total = sum(scores.values()) / len(scores)

                scoring["agents"].append(
                    {
                        "name": agent["agent_name"],
                        "role": agent.get("military_role"),
                        "model": agent.get("model"),
                        "provider": agent.get("provider"),
                        "execution_time": agent.get("execution_time"),
                        "response_length": agent.get("response_length"),
                        "scores": scores,
                        "total_score": round(total, 2),
                    }
                )
            else:
                scoring["agents"].append(
                    {
                        "name": agent.get("agent_name"),
                        "status": "FAILED",
                        "error": agent.get("error", "Unknown")[:100],
                    }
                )

        # Rank successful agents
        successful = [a for a in scoring["agents"] if "total_score" in a]
        successful.sort(key=lambda x: x["total_score"], reverse=True)
        for i, agent in enumerate(successful):
            agent["rank"] = i + 1

        return scoring


async def main():
    """Execute military swarm"""
    swarm = ArtemisMilitarySwarmDirect()

    print("‚öîÔ∏è ARTEMIS MILITARY COMMAND INITIALIZED")
    print("üéØ Target: sophia-intel-ai repository")
    print("üìä Mission: Full tactical analysis\n")

    # Deploy swarm
    results = await swarm.deploy_swarm()

    # Display results
    print("\n" + "=" * 70)
    print(" MISSION COMPLETE - DEBRIEFING")
    print("=" * 70)

    successful = 0
    failed = 0

    for agent in results["agents"]:
        if "error" not in agent:
            successful += 1
            print(f"\n‚úÖ {agent['agent_name']} ({agent['military_role']})")
            print(f"   Model: {agent['model']}")
            print(f"   Provider: {agent['provider']}")
            print(f"   Time: {agent['execution_time']:.1f}s")
            print(f"   Intel: {agent['response_length']:,} chars")
        else:
            failed += 1
            print(f"\n‚ùå {agent['agent_name']} ({agent.get('military_role', 'UNKNOWN')})")
            print(f"   Error: {agent['error'][:100]}")

    # Score agents
    scoring = swarm.score_agents(results)

    print("\n" + "=" * 70)
    print(" PERFORMANCE SCORING")
    print("=" * 70)

    for agent in scoring["agents"]:
        if "total_score" in agent:
            print(f"\nüéñÔ∏è {agent['name']} - Rank #{agent.get('rank', '?')}")
            print(f"   Total Score: {agent['total_score']}/10")
            print(f"   Speed: {agent['scores']['speed']:.1f}/10")
            print(f"   Quality: {agent['scores']['quality']:.1f}/10")
            print(f"   Communication: {agent['scores']['communication']}/10")

    # Save scoring
    scoring_file = f"artemis_scoring_direct_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
    with open(scoring_file, "w") as f:
        json.dump(scoring, f, indent=2)

    print("\nüìä Summary:")
    print(f"  ‚Ä¢ Successful: {successful}/4")
    print(f"  ‚Ä¢ Failed: {failed}/4")
    print(f"  ‚Ä¢ Total Time: {results['total_execution_time']:.1f}s")
    print(f"  ‚Ä¢ Scoring Report: {scoring_file}")

    return results, scoring


if __name__ == "__main__":
    print("‚öîÔ∏è ARTEMIS MILITARY SWARM ACTIVATION")
    print("=" * 70)

    # Set Gemini API key if provided
    if "GOOGLE_API_KEY" not in os.environ:
        os.environ["GOOGLE_API_KEY"] = "AIzaSyA0rewcfUHo87WMEz4a8Og1eAWTslxlgEE"

    results, scores = asyncio.run(main())
