#!/usr/bin/env python3
"""SOPHIA V4 Ultimate Ecosystem - Complete Autonomous Partner for CEO ðŸ¤ ðŸ”¥
Repository: https://github.com/ai-cherry/sophia-intel
Fly.io: spring-flower-2097, 17817d62b53418, ord
Lambda: gpu_1x_gh200, 192.222.51.223, 192.222.50.242, us-east-3
"""
from fastapi import FastAPI, HTTPException, BackgroundTasks, WebSocket, WebSocketDisconnect
from fastapi.middleware.cors import CORSMiddleware
from fastapi.staticfiles import StaticFiles
from pydantic import BaseModel
from datetime import datetime, timedelta
import requests, os, logging, uuid, subprocess, asyncio, json, glob, paramiko, time
from github import Github
from qdrant_client import QdrantClient
from qdrant_client.models import PointStruct, Distance, VectorParams, Filter, FieldCondition, MatchValue
from mem0 import Memory
from langchain_community.tools import TavilySearchResults
from langgraph.prebuilt import create_react_agent
from langchain_openai import ChatOpenAI
from langchain.schema import HumanMessage, SystemMessage
import sentry_sdk
from tenacity import retry, stop_after_attempt, wait_exponential
import threading
from typing import Dict, List, Optional
import hashlib
import base64

# Initialize services
sentry_sdk.init(dsn=os.getenv('SENTRY_DSN'))
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

app = FastAPI(
    title="SOPHIA V4 Ultimate Ecosystem",
    description="Complete autonomous partner for CEO with full infrastructure control",
    version="4.0.0-ULTIMATE-ECOSYSTEM"
)

# CORS middleware
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Initialize integrations
g = Github(os.getenv('GITHUB_TOKEN'))
org = g.get_organization('ai-cherry')
sophia_repo = g.get_repo('ai-cherry/sophia-intel')
qdrant = QdrantClient(url=os.getenv('QDRANT_URL'), api_key=os.getenv('QDRANT_API_KEY'))
mem0 = Memory()
tavily = TavilySearchResults(api_key=os.getenv('TAVILY_API_KEY'))

# Lambda Labs configuration
LAMBDA_IPS = ['192.222.51.223', '192.222.50.242']
LAMBDA_API_KEY = os.getenv('LAMBDA_API_KEY')
LAMBDA_SSH_KEY_PATH = '/secrets/lambda_ssh_key'

# Fly.io configuration
FLY_APP_NAME = 'sophia-intel'
FLY_MACHINE_ID = '17817d62b53418'
FLY_REGION = 'ord'

# OpenRouter model configuration - CEO-approved models only
OPENROUTER_MODELS = {
    'primary': 'anthropic/claude-3-5-sonnet-20241022',  # Claude Sonnet 4
    'speed': 'google/gemini-2.0-flash-exp',             # Gemini 2.0 Flash
    'coding': 'deepseek/deepseek-v3',                   # DeepSeek V3
    'coder': 'qwen/qwen-2.5-coder-32b-instruct',       # Qwen3 Coder
    'fallback': 'openai/gpt-4o-mini'                    # GPT-4o-mini
}

# Initialize Qdrant collections
try:
    qdrant.create_collection(
        collection_name="sophia_memory",
        vectors_config=VectorParams(size=1536, distance=Distance.COSINE)
    )
    qdrant.create_collection(
        collection_name="repository_index",
        vectors_config=VectorParams(size=1536, distance=Distance.COSINE)
    )
    qdrant.create_collection(
        collection_name="infrastructure_state",
        vectors_config=VectorParams(size=1536, distance=Distance.COSINE)
    )
except:
    pass  # Collections exist

# Global state management
class SOPHIAState:
    def __init__(self):
        self.active_connections: Dict[str, WebSocket] = {}
        self.infrastructure_status = {}
        self.repository_index = {}
        self.agent_swarms = {}
        self.health_monitors = {}
        self.last_repo_sync = None
        
    async def broadcast_update(self, message: dict):
        """Broadcast updates to all connected clients"""
        for connection in self.active_connections.values():
            try:
                await connection.send_json(message)
            except:
                pass  # Connection closed

sophia_state = SOPHIAState()

# Request models
class ChatRequest(BaseModel):
    query: str
    user_id: str
    context: Optional[dict] = None
    priority: str = 'normal'

class InfrastructureRequest(BaseModel):
    action: str
    target: str
    parameters: Optional[dict] = None
    user_id: str

class RepositoryRequest(BaseModel):
    action: str
    repository: str
    parameters: Optional[dict] = None
    user_id: str

class AgentSwarmRequest(BaseModel):
    task: str
    agents: List[str]
    parameters: dict
    user_id: str

# OpenRouter API integration with CEO-approved models
async def call_openrouter_model(model_key: str, messages: list, max_tokens: int = 3000, temperature: float = 0.7):
    """Call OpenRouter API with CEO-approved models only"""
    model = OPENROUTER_MODELS.get(model_key, OPENROUTER_MODELS['fallback'])
    try:
        response = requests.post(
            'https://openrouter.ai/api/v1/chat/completions',
            headers={
                'Authorization': f'Bearer {os.getenv("OPENROUTER_API_KEY")}',
                'Content-Type': 'application/json'
            },
            json={
                'model': model,
                'messages': messages,
                'max_tokens': max_tokens,
                'temperature': temperature
            },
            timeout=45
        )
        if response.status_code == 200:
            content = response.json().get('choices', [{}])[0].get('message', {}).get('content', '')
            return {
                'content': content,
                'model': model,
                'timestamp': datetime.now().isoformat(),
                'success': True
            }
        logger.error(f"OpenRouter error: {response.status_code} - {response.text}")
        return {
            'content': '',
            'error': f'OpenRouter error: {response.status_code}',
            'model': model,
            'timestamp': datetime.now().isoformat(),
            'success': False
        }
    except Exception as e:
        sentry_sdk.capture_exception(e)
        return {
            'content': '',
            'error': str(e),
            'model': model,
            'timestamp': datetime.now().isoformat(),
            'success': False
        }

# Memory management with auto-indexing
async def store_conversation_memory(user_id: str, query: str, response: dict, context: dict = None):
    """Store conversation in Qdrant with rich context"""
    try:
        text = f"Query: {query}\nResponse: {json.dumps(response)}\nContext: {json.dumps(context or {})}"
        
        # Simple embedding generation (replace with proper embedding model)
        embedding = []
        for i in range(0, min(len(text), 1536), 10):
            chunk = text[i:i+10]
            hash_val = hashlib.md5(chunk.encode()).hexdigest()
            embedding.append(int(hash_val[:8], 16) % 1000 / 1000.0)
        
        # Pad to 1536 dimensions
        while len(embedding) < 1536:
            embedding.append(0.0)
        embedding = embedding[:1536]
        
        point = PointStruct(
            id=str(uuid.uuid4()),
            vector=embedding,
            payload={
                'user_id': user_id,
                'query': query,
                'response': response,
                'context': context or {},
                'timestamp': datetime.now().isoformat(),
                'type': 'conversation'
            }
        )
        qdrant.upsert(collection_name="sophia_memory", points=[point])
        
        # Broadcast memory update
        await sophia_state.broadcast_update({
            'type': 'memory_update',
            'user_id': user_id,
            'timestamp': datetime.now().isoformat()
        })
        
        return {'status': 'Memory stored successfully', 'timestamp': datetime.now().isoformat()}
    except Exception as e:
        sentry_sdk.capture_exception(e)
        return {'status': 'Memory storage failed', 'error': str(e), 'timestamp': datetime.now().isoformat()}

async def retrieve_relevant_memories(user_id: str, query: str, limit: int = 5):
    """Retrieve relevant memories from Qdrant"""
    try:
        # Simple query embedding
        query_embedding = []
        for i in range(0, min(len(query), 1536), 10):
            chunk = query[i:i+10]
            hash_val = hashlib.md5(chunk.encode()).hexdigest()
            query_embedding.append(int(hash_val[:8], 16) % 1000 / 1000.0)
        
        while len(query_embedding) < 1536:
            query_embedding.append(0.0)
        query_embedding = query_embedding[:1536]
        
        results = qdrant.search(
            collection_name="sophia_memory",
            query_vector=query_embedding,
            query_filter=Filter(
                must=[
                    FieldCondition(key="user_id", match=MatchValue(value=user_id)),
                    FieldCondition(key="type", match=MatchValue(value="conversation"))
                ]
            ),
            limit=limit
        )
        return [result.payload for result in results]
    except Exception as e:
        sentry_sdk.capture_exception(e)
        return []

# Repository indexing and monitoring
async def index_repository_structure():
    """Index complete repository structure with auto-updates"""
    try:
        logger.info("Starting repository indexing...")
        
        # Get all repositories in ai-cherry organization
        repos = list(org.get_repos())
        
        for repo in repos:
            try:
                # Get repository contents
                contents = repo.get_contents('', ref='main')
                repo_structure = await analyze_repository_contents(repo, contents)
                
                # Store in Qdrant
                text = f"Repository: {repo.name}\nStructure: {json.dumps(repo_structure)}"
                embedding = []
                for i in range(0, min(len(text), 1536), 10):
                    chunk = text[i:i+10]
                    hash_val = hashlib.md5(chunk.encode()).hexdigest()
                    embedding.append(int(hash_val[:8], 16) % 1000 / 1000.0)
                
                while len(embedding) < 1536:
                    embedding.append(0.0)
                embedding = embedding[:1536]
                
                point = PointStruct(
                    id=f"repo_{repo.name}",
                    vector=embedding,
                    payload={
                        'repository': repo.name,
                        'structure': repo_structure,
                        'last_updated': datetime.now().isoformat(),
                        'type': 'repository_index'
                    }
                )
                qdrant.upsert(collection_name="repository_index", points=[point])
                
                sophia_state.repository_index[repo.name] = repo_structure
                
            except Exception as e:
                logger.error(f"Failed to index repository {repo.name}: {e}")
        
        sophia_state.last_repo_sync = datetime.now()
        
        # Broadcast repository update
        await sophia_state.broadcast_update({
            'type': 'repository_index_update',
            'repositories': list(sophia_state.repository_index.keys()),
            'timestamp': datetime.now().isoformat()
        })
        
        logger.info(f"Repository indexing complete. Indexed {len(repos)} repositories.")
        return {'status': 'Repository indexing complete', 'repositories': len(repos)}
        
    except Exception as e:
        logger.error(f"Repository indexing failed: {e}")
        sentry_sdk.capture_exception(e)
        return {'status': 'Repository indexing failed', 'error': str(e)}

async def analyze_repository_contents(repo, contents, path=''):
    """Analyze repository contents recursively"""
    structure = {
        'files': [],
        'directories': [],
        'python_files': [],
        'config_files': [],
        'dependencies': [],
        'issues': [],
        'last_commit': None
    }
    
    try:
        # Get last commit
        commits = list(repo.get_commits())
        if commits:
            structure['last_commit'] = {
                'sha': commits[0].sha,
                'message': commits[0].commit.message,
                'date': commits[0].commit.author.date.isoformat()
            }
        
        for item in contents:
            if item.type == 'dir':
                structure['directories'].append(item.path)
                try:
                    # Recursively analyze subdirectories (limit depth)
                    if path.count('/') < 3:
                        sub_contents = repo.get_contents(item.path, ref='main')
                        sub_structure = await analyze_repository_contents(repo, sub_contents, item.path)
                        structure['files'].extend(sub_structure['files'])
                        structure['python_files'].extend(sub_structure['python_files'])
                        structure['config_files'].extend(sub_structure['config_files'])
                        structure['issues'].extend(sub_structure['issues'])
                except:
                    pass  # Skip if can't access directory
            else:
                structure['files'].append({
                    'path': item.path,
                    'size': item.size,
                    'sha': item.sha
                })
                
                # Analyze Python files
                if item.path.endswith('.py'):
                    structure['python_files'].append(item.path)
                    try:
                        content = item.decoded_content.decode('utf-8')
                        # Check for common issues
                        if 'langchain' in content.lower() and '0.2' in content:
                            structure['issues'].append(f"{item.path}: Outdated LangChain version detected")
                        if 'requests' in content and 'tenacity' not in content:
                            structure['issues'].append(f"{item.path}: Missing retry logic for API calls")
                    except:
                        pass
                
                # Analyze config files
                elif item.path in ['requirements.txt', 'pyproject.toml', 'package.json', 'fly.toml', 'Dockerfile']:
                    structure['config_files'].append(item.path)
                    if item.path == 'requirements.txt':
                        try:
                            content = item.decoded_content.decode('utf-8')
                            structure['dependencies'] = [dep.strip() for dep in content.splitlines() if dep.strip()]
                        except:
                            pass
        
        return structure
        
    except Exception as e:
        logger.error(f"Failed to analyze repository contents: {e}")
        return structure

# Infrastructure monitoring and control
async def monitor_infrastructure_health():
    """Monitor all infrastructure components"""
    try:
        health_status = {
            'fly_io': await check_flyio_health(),
            'lambda_labs': await check_lambda_health(),
            'github': await check_github_health(),
            'apis': await check_api_health(),
            'timestamp': datetime.now().isoformat()
        }
        
        sophia_state.infrastructure_status = health_status
        
        # Check for critical issues
        critical_issues = []
        for service, status in health_status.items():
            if isinstance(status, dict) and not status.get('healthy', True):
                critical_issues.append(f"{service}: {status.get('error', 'Unknown issue')}")
        
        if critical_issues:
            await sophia_state.broadcast_update({
                'type': 'infrastructure_alert',
                'issues': critical_issues,
                'timestamp': datetime.now().isoformat()
            })
        
        return health_status
        
    except Exception as e:
        logger.error(f"Infrastructure monitoring failed: {e}")
        sentry_sdk.capture_exception(e)
        return {'error': str(e), 'timestamp': datetime.now().isoformat()}

async def check_flyio_health():
    """Check Fly.io application health"""
    try:
        # Check machine status
        result = subprocess.run(
            ['flyctl', 'status', '--app', FLY_APP_NAME],
            capture_output=True,
            text=True,
            timeout=30
        )
        
        if result.returncode == 0:
            return {
                'healthy': True,
                'status': 'running',
                'details': result.stdout,
                'timestamp': datetime.now().isoformat()
            }
        else:
            return {
                'healthy': False,
                'error': result.stderr,
                'timestamp': datetime.now().isoformat()
            }
    except Exception as e:
        return {
            'healthy': False,
            'error': str(e),
            'timestamp': datetime.now().isoformat()
        }

async def check_lambda_health():
    """Check Lambda Labs GPU health"""
    try:
        lambda_status = []
        for ip in LAMBDA_IPS:
            try:
                ssh = paramiko.SSHClient()
                ssh.set_missing_host_key_policy(paramiko.AutoAddPolicy())
                key = paramiko.RSAKey.from_private_key_file(LAMBDA_SSH_KEY_PATH)
                ssh.connect(ip, username='ubuntu', pkey=key, timeout=10)
                
                stdin, stdout, stderr = ssh.exec_command('nvidia-smi --query-gpu=name,utilization.gpu,memory.used,memory.total --format=csv,noheader,nounits', timeout=15)
                gpu_stats = stdout.read().decode('utf-8').strip()
                ssh.close()
                
                lambda_status.append({
                    'ip': ip,
                    'healthy': True,
                    'gpu_stats': gpu_stats,
                    'timestamp': datetime.now().isoformat()
                })
            except Exception as e:
                lambda_status.append({
                    'ip': ip,
                    'healthy': False,
                    'error': str(e),
                    'timestamp': datetime.now().isoformat()
                })
        
        return {
            'healthy': all(status['healthy'] for status in lambda_status),
            'gpus': lambda_status,
            'timestamp': datetime.now().isoformat()
        }
    except Exception as e:
        return {
            'healthy': False,
            'error': str(e),
            'timestamp': datetime.now().isoformat()
        }

async def check_github_health():
    """Check GitHub API and repository health"""
    try:
        # Test GitHub API access
        rate_limit = g.get_rate_limit()
        
        # Check repository access
        repos = list(org.get_repos())
        
        return {
            'healthy': True,
            'rate_limit': {
                'core': rate_limit.core.remaining,
                'search': rate_limit.search.remaining
            },
            'repositories': len(repos),
            'timestamp': datetime.now().isoformat()
        }
    except Exception as e:
        return {
            'healthy': False,
            'error': str(e),
            'timestamp': datetime.now().isoformat()
        }

async def check_api_health():
    """Check external API health"""
    try:
        api_status = {}
        
        # Check OpenRouter
        try:
            response = requests.get('https://openrouter.ai/api/v1/models', timeout=10)
            api_status['openrouter'] = {'healthy': response.status_code == 200}
        except:
            api_status['openrouter'] = {'healthy': False}
        
        # Check Tavily
        try:
            results = tavily.run("test query")
            api_status['tavily'] = {'healthy': True}
        except:
            api_status['tavily'] = {'healthy': False}
        
        # Check Qdrant
        try:
            collections = qdrant.get_collections()
            api_status['qdrant'] = {'healthy': True, 'collections': len(collections.collections)}
        except:
            api_status['qdrant'] = {'healthy': False}
        
        return {
            'healthy': all(status['healthy'] for status in api_status.values()),
            'apis': api_status,
            'timestamp': datetime.now().isoformat()
        }
    except Exception as e:
        return {
            'healthy': False,
            'error': str(e),
            'timestamp': datetime.now().isoformat()
        }

# Infrastructure control operations
async def control_flyio_infrastructure(action: str, parameters: dict):
    """Control Fly.io infrastructure"""
    try:
        if action == 'scale':
            count = parameters.get('count', 1)
            result = subprocess.run(
                ['flyctl', 'scale', 'count', str(count), '--app', FLY_APP_NAME],
                capture_output=True,
                text=True,
                timeout=60
            )
            return {
                'success': result.returncode == 0,
                'output': result.stdout,
                'error': result.stderr if result.returncode != 0 else None,
                'timestamp': datetime.now().isoformat()
            }
        
        elif action == 'deploy':
            result = subprocess.run(
                ['flyctl', 'deploy', '--app', FLY_APP_NAME],
                capture_output=True,
                text=True,
                timeout=300
            )
            return {
                'success': result.returncode == 0,
                'output': result.stdout,
                'error': result.stderr if result.returncode != 0 else None,
                'timestamp': datetime.now().isoformat()
            }
        
        elif action == 'restart':
            result = subprocess.run(
                ['flyctl', 'machine', 'restart', FLY_MACHINE_ID, '--app', FLY_APP_NAME],
                capture_output=True,
                text=True,
                timeout=60
            )
            return {
                'success': result.returncode == 0,
                'output': result.stdout,
                'error': result.stderr if result.returncode != 0 else None,
                'timestamp': datetime.now().isoformat()
            }
        
        elif action == 'update_secret':
            secret_name = parameters.get('name')
            secret_value = parameters.get('value')
            result = subprocess.run(
                ['flyctl', 'secrets', 'set', f'{secret_name}={secret_value}', '--app', FLY_APP_NAME],
                capture_output=True,
                text=True,
                timeout=60
            )
            return {
                'success': result.returncode == 0,
                'output': result.stdout,
                'error': result.stderr if result.returncode != 0 else None,
                'timestamp': datetime.now().isoformat()
            }
        
        else:
            return {
                'success': False,
                'error': f'Unknown action: {action}',
                'timestamp': datetime.now().isoformat()
            }
            
    except Exception as e:
        logger.error(f"Fly.io control failed: {e}")
        sentry_sdk.capture_exception(e)
        return {
            'success': False,
            'error': str(e),
            'timestamp': datetime.now().isoformat()
        }

async def control_lambda_infrastructure(action: str, parameters: dict):
    """Control Lambda Labs infrastructure"""
    try:
        if action == 'run_task':
            task_type = parameters.get('task_type')
            data = parameters.get('data', {})
            
            # Select GPU based on load balancing
            lambda_ip = LAMBDA_IPS[hash(task_type) % len(LAMBDA_IPS)]
            
            ssh = paramiko.SSHClient()
            ssh.set_missing_host_key_policy(paramiko.AutoAddPolicy())
            key = paramiko.RSAKey.from_private_key_file(LAMBDA_SSH_KEY_PATH)
            ssh.connect(lambda_ip, username='ubuntu', pkey=key, timeout=30)
            
            # Execute ML task
            task_data = json.dumps(data).replace('"', '\\"')
            command = f'python3 /home/ubuntu/ml_tasks/sophia_ml.py --task {task_type} --data "{task_data}"'
            
            stdin, stdout, stderr = ssh.exec_command(command, timeout=300)
            result = stdout.read().decode('utf-8')
            error = stderr.read().decode('utf-8')
            ssh.close()
            
            if error:
                return {
                    'success': False,
                    'error': error,
                    'lambda_ip': lambda_ip,
                    'timestamp': datetime.now().isoformat()
                }
            
            return {
                'success': True,
                'result': json.loads(result) if result else {},
                'lambda_ip': lambda_ip,
                'timestamp': datetime.now().isoformat()
            }
        
        elif action == 'sync_data':
            sync_results = []
            for ip in LAMBDA_IPS:
                try:
                    rsync_cmd = f"rsync -avz -e 'ssh -i {LAMBDA_SSH_KEY_PATH}' /data/models/ ubuntu@{ip}:/home/ubuntu/models/"
                    result = subprocess.run(rsync_cmd, shell=True, capture_output=True, text=True, timeout=300)
                    
                    sync_results.append({
                        'ip': ip,
                        'success': result.returncode == 0,
                        'output': result.stdout,
                        'error': result.stderr if result.returncode != 0 else None
                    })
                except Exception as e:
                    sync_results.append({
                        'ip': ip,
                        'success': False,
                        'error': str(e)
                    })
            
            return {
                'success': all(result['success'] for result in sync_results),
                'sync_results': sync_results,
                'timestamp': datetime.now().isoformat()
            }
        
        else:
            return {
                'success': False,
                'error': f'Unknown action: {action}',
                'timestamp': datetime.now().isoformat()
            }
            
    except Exception as e:
        logger.error(f"Lambda control failed: {e}")
        sentry_sdk.capture_exception(e)
        return {
            'success': False,
            'error': str(e),
            'timestamp': datetime.now().isoformat()
        }

# GitHub operations with thoughtful merging
async def execute_github_operation(action: str, repository: str, parameters: dict):
    """Execute GitHub operations with thoughtful merging"""
    try:
        repo = org.get_repo(repository)
        
        if action == 'create_branch':
            branch_name = parameters.get('branch_name')
            base_branch = parameters.get('base_branch', 'main')
            
            base_sha = repo.get_branch(base_branch).commit.sha
            repo.create_git_ref(ref=f'refs/heads/{branch_name}', sha=base_sha)
            
            return {
                'success': True,
                'branch_name': branch_name,
                'base_sha': base_sha,
                'timestamp': datetime.now().isoformat()
            }
        
        elif action == 'commit_file':
            file_path = parameters.get('file_path')
            content = parameters.get('content')
            message = parameters.get('message')
            branch = parameters.get('branch', 'main')
            
            try:
                # Try to update existing file
                file = repo.get_contents(file_path, ref=branch)
                repo.update_file(
                    file_path,
                    message,
                    content,
                    file.sha,
                    branch=branch
                )
            except:
                # Create new file
                repo.create_file(
                    file_path,
                    message,
                    content,
                    branch=branch
                )
            
            return {
                'success': True,
                'file_path': file_path,
                'message': message,
                'branch': branch,
                'timestamp': datetime.now().isoformat()
            }
        
        elif action == 'create_pr':
            title = parameters.get('title')
            body = parameters.get('body')
            head_branch = parameters.get('head_branch')
            base_branch = parameters.get('base_branch', 'main')
            
            pr = repo.create_pull(
                title=title,
                body=body,
                head=head_branch,
                base=base_branch
            )
            
            return {
                'success': True,
                'pr_number': pr.number,
                'pr_url': pr.html_url,
                'timestamp': datetime.now().isoformat()
            }
        
        elif action == 'merge_pr':
            pr_number = parameters.get('pr_number')
            merge_method = parameters.get('merge_method', 'squash')
            
            pr = repo.get_pull(pr_number)
            
            # Check if PR is mergeable
            if not pr.mergeable:
                return {
                    'success': False,
                    'error': 'PR is not mergeable',
                    'timestamp': datetime.now().isoformat()
                }
            
            # Merge PR
            merge_result = pr.merge(merge_method=merge_method)
            
            return {
                'success': merge_result.merged,
                'merge_sha': merge_result.sha,
                'timestamp': datetime.now().isoformat()
            }
        
        elif action == 'analyze_repository':
            contents = repo.get_contents('', ref='main')
            structure = await analyze_repository_contents(repo, contents)
            
            return {
                'success': True,
                'repository': repository,
                'structure': structure,
                'timestamp': datetime.now().isoformat()
            }
        
        else:
            return {
                'success': False,
                'error': f'Unknown action: {action}',
                'timestamp': datetime.now().isoformat()
            }
            
    except Exception as e:
        logger.error(f"GitHub operation failed: {e}")
        sentry_sdk.capture_exception(e)
        return {
            'success': False,
            'error': str(e),
            'timestamp': datetime.now().isoformat()
        }

# AI Agent Swarms with Agno framework
async def create_agent_swarm(task: str, agents: List[str], parameters: dict):
    """Create and manage AI agent swarms"""
    try:
        swarm_id = str(uuid.uuid4())
        
        # Initialize agents based on task type
        agent_configs = {
            'researcher': {
                'model': 'primary',
                'role': 'Research and gather information',
                'tools': ['tavily', 'github_search']
            },
            'coder': {
                'model': 'coding',
                'role': 'Write and review code',
                'tools': ['github_api', 'code_analysis']
            },
            'infrastructure': {
                'model': 'speed',
                'role': 'Manage infrastructure',
                'tools': ['flyio_api', 'lambda_api']
            },
            'analyst': {
                'model': 'primary',
                'role': 'Analyze data and provide insights',
                'tools': ['qdrant', 'mem0']
            }
        }
        
        swarm_agents = []
        for agent_type in agents:
            if agent_type in agent_configs:
                config = agent_configs[agent_type]
                swarm_agents.append({
                    'id': str(uuid.uuid4()),
                    'type': agent_type,
                    'config': config,
                    'status': 'initialized'
                })
        
        sophia_state.agent_swarms[swarm_id] = {
            'task': task,
            'agents': swarm_agents,
            'parameters': parameters,
            'status': 'created',
            'created_at': datetime.now().isoformat()
        }
        
        # Start swarm execution in background
        asyncio.create_task(execute_agent_swarm(swarm_id))
        
        return {
            'success': True,
            'swarm_id': swarm_id,
            'agents': len(swarm_agents),
            'timestamp': datetime.now().isoformat()
        }
        
    except Exception as e:
        logger.error(f"Agent swarm creation failed: {e}")
        sentry_sdk.capture_exception(e)
        return {
            'success': False,
            'error': str(e),
            'timestamp': datetime.now().isoformat()
        }

async def execute_agent_swarm(swarm_id: str):
    """Execute agent swarm tasks"""
    try:
        swarm = sophia_state.agent_swarms.get(swarm_id)
        if not swarm:
            return
        
        swarm['status'] = 'executing'
        
        # Execute agents in parallel
        tasks = []
        for agent in swarm['agents']:
            task = execute_agent_task(agent, swarm['task'], swarm['parameters'])
            tasks.append(task)
        
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # Compile results
        swarm_results = []
        for i, result in enumerate(results):
            if isinstance(result, Exception):
                swarm_results.append({
                    'agent_id': swarm['agents'][i]['id'],
                    'success': False,
                    'error': str(result)
                })
            else:
                swarm_results.append({
                    'agent_id': swarm['agents'][i]['id'],
                    'success': True,
                    'result': result
                })
        
        swarm['results'] = swarm_results
        swarm['status'] = 'completed'
        swarm['completed_at'] = datetime.now().isoformat()
        
        # Broadcast swarm completion
        await sophia_state.broadcast_update({
            'type': 'agent_swarm_completed',
            'swarm_id': swarm_id,
            'results': swarm_results,
            'timestamp': datetime.now().isoformat()
        })
        
    except Exception as e:
        logger.error(f"Agent swarm execution failed: {e}")
        sentry_sdk.capture_exception(e)
        if swarm_id in sophia_state.agent_swarms:
            sophia_state.agent_swarms[swarm_id]['status'] = 'failed'
            sophia_state.agent_swarms[swarm_id]['error'] = str(e)

async def execute_agent_task(agent: dict, task: str, parameters: dict):
    """Execute individual agent task"""
    try:
        agent_type = agent['type']
        config = agent['config']
        
        # Prepare agent prompt
        system_prompt = f"""You are a {agent_type} agent with the role: {config['role']}
        
        Task: {task}
        Parameters: {json.dumps(parameters)}
        
        Available tools: {', '.join(config['tools'])}
        
        Provide a detailed response with actionable insights and specific recommendations.
        """
        
        # Call appropriate model
        response = await call_openrouter_model(
            config['model'],
            [
                {'role': 'system', 'content': system_prompt},
                {'role': 'user', 'content': task}
            ],
            max_tokens=2000
        )
        
        return {
            'agent_type': agent_type,
            'response': response,
            'timestamp': datetime.now().isoformat()
        }
        
    except Exception as e:
        logger.error(f"Agent task execution failed: {e}")
        raise e

# Deep web research capabilities
async def conduct_deep_web_research(query: str, sources_limit: int = 5):
    """Conduct comprehensive deep web research"""
    try:
        # Use Tavily for deep web search
        web_results = tavily.run(query)
        
        if not isinstance(web_results, list):
            web_results = [web_results]
        
        # Limit results
        web_results = web_results[:sources_limit]
        
        # Enhance results with additional context
        enhanced_results = []
        for result in web_results:
            if isinstance(result, dict):
                enhanced_results.append({
                    'title': result.get('title', 'No title'),
                    'url': result.get('url', ''),
                    'content': result.get('content', ''),
                    'score': result.get('score', 0),
                    'timestamp': datetime.now().isoformat()
                })
            else:
                enhanced_results.append({
                    'content': str(result),
                    'timestamp': datetime.now().isoformat()
                })
        
        return {
            'query': query,
            'results': enhanced_results,
            'sources_count': len(enhanced_results),
            'timestamp': datetime.now().isoformat()
        }
        
    except Exception as e:
        logger.error(f"Deep web research failed: {e}")
        sentry_sdk.capture_exception(e)
        return {
            'query': query,
            'results': [],
            'error': str(e),
            'timestamp': datetime.now().isoformat()
        }

# Main autonomous chat endpoint
@app.post("/api/v1/chat")
async def autonomous_chat(request: ChatRequest, background_tasks: BackgroundTasks):
    """Ultimate autonomous chat - CEO's complete partner"""
    try:
        start_time = time.time()
        
        # Retrieve relevant memories
        memories = await retrieve_relevant_memories(request.user_id, request.query)
        
        # Classify query intent
        query_lower = request.query.lower()
        
        # Infrastructure control queries
        if any(kw in query_lower for kw in ['scale', 'deploy', 'restart', 'secret', 'infrastructure', 'fly.io', 'lambda']):
            if 'fly.io' in query_lower or 'fly' in query_lower:
                action = 'scale' if 'scale' in query_lower else 'deploy' if 'deploy' in query_lower else 'restart' if 'restart' in query_lower else 'status'
                parameters = {}
                
                if 'scale' in query_lower:
                    # Extract scale count
                    words = request.query.split()
                    for i, word in enumerate(words):
                        if word.isdigit():
                            parameters['count'] = int(word)
                            break
                
                result = await control_flyio_infrastructure(action, parameters)
                
                response = f"ðŸ¤  Yo, partner! Fly.io {action} operation: {'Success!' if result['success'] else 'Failed!'}\n\n"
                if result['success']:
                    response += f"**Output**: {result['output'][:200]}...\n"
                else:
                    response += f"**Error**: {result['error']}\n"
                
                response += "That's the real fucking deal from Fly.io control! ðŸš€"
                
                await store_conversation_memory(request.user_id, request.query, result, {'action': 'infrastructure_control'})
                
                return {
                    'message': response,
                    'infrastructure_result': result,
                    'action': 'infrastructure_control',
                    'response_time': f"{time.time() - start_time:.2f}s",
                    'timestamp': datetime.now().isoformat()
                }
            
            elif 'lambda' in query_lower:
                action = 'run_task' if 'task' in query_lower else 'sync_data'
                parameters = {'task_type': 'general', 'data': {'query': request.query}}
                
                result = await control_lambda_infrastructure(action, parameters)
                
                response = f"ðŸ¤  Yo, partner! Lambda GPU {action} operation: {'Success!' if result['success'] else 'Failed!'}\n\n"
                if result['success']:
                    response += f"**Result**: {json.dumps(result.get('result', {}), indent=2)[:200]}...\n"
                else:
                    response += f"**Error**: {result['error']}\n"
                
                response += "That's the real fucking deal from Lambda GPU power! ðŸ”¥"
                
                await store_conversation_memory(request.user_id, request.query, result, {'action': 'lambda_control'})
                
                return {
                    'message': response,
                    'lambda_result': result,
                    'action': 'lambda_control',
                    'response_time': f"{time.time() - start_time:.2f}s",
                    'timestamp': datetime.now().isoformat()
                }
        
        # Repository and code management queries
        elif any(kw in query_lower for kw in ['repository', 'repo', 'github', 'code', 'commit', 'merge', 'pr', 'pull request']):
            # Determine repository
            repository = 'sophia-intel'  # Default
            if 'ai-cherry/' in request.query:
                repo_match = request.query.split('ai-cherry/')[1].split()[0]
                repository = repo_match
            
            # Determine action
            if 'analyze' in query_lower or 'structure' in query_lower:
                action = 'analyze_repository'
            elif 'commit' in query_lower:
                action = 'commit_file'
            elif 'pr' in query_lower or 'pull request' in query_lower:
                action = 'create_pr'
            else:
                action = 'analyze_repository'
            
            parameters = {'repository': repository}
            
            result = await execute_github_operation(action, repository, parameters)
            
            response = f"ðŸ¤  Yo, partner! GitHub {action} for {repository}: {'Success!' if result['success'] else 'Failed!'}\n\n"
            if result['success']:
                if action == 'analyze_repository':
                    structure = result.get('structure', {})
                    response += f"**Repository Analysis**:\n"
                    response += f"- Python files: {len(structure.get('python_files', []))}\n"
                    response += f"- Dependencies: {len(structure.get('dependencies', []))}\n"
                    response += f"- Issues found: {len(structure.get('issues', []))}\n"
                    if structure.get('issues'):
                        response += f"- Top issues: {', '.join(structure['issues'][:3])}\n"
                else:
                    response += f"**Result**: {json.dumps(result, indent=2)[:200]}...\n"
            else:
                response += f"**Error**: {result['error']}\n"
            
            response += "That's the real fucking deal from GitHub operations! ðŸ™"
            
            await store_conversation_memory(request.user_id, request.query, result, {'action': 'github_operation'})
            
            return {
                'message': response,
                'github_result': result,
                'action': 'github_operation',
                'response_time': f"{time.time() - start_time:.2f}s",
                'timestamp': datetime.now().isoformat()
            }
        
        # Agent swarm queries
        elif any(kw in query_lower for kw in ['agent', 'swarm', 'team', 'collaborate']):
            # Determine agents needed
            agents = []
            if 'research' in query_lower:
                agents.append('researcher')
            if 'code' in query_lower or 'programming' in query_lower:
                agents.append('coder')
            if 'infrastructure' in query_lower or 'deploy' in query_lower:
                agents.append('infrastructure')
            if 'analyze' in query_lower or 'data' in query_lower:
                agents.append('analyst')
            
            if not agents:
                agents = ['researcher', 'analyst']  # Default swarm
            
            parameters = {'priority': request.priority, 'context': request.context}
            
            result = await create_agent_swarm(request.query, agents, parameters)
            
            response = f"ðŸ¤  Yo, partner! Agent swarm created: {'Success!' if result['success'] else 'Failed!'}\n\n"
            if result['success']:
                response += f"**Swarm ID**: {result['swarm_id']}\n"
                response += f"**Agents**: {result['agents']} agents deployed\n"
                response += f"**Status**: Executing in background\n"
            else:
                response += f"**Error**: {result['error']}\n"
            
            response += "That's the real fucking deal from AI agent swarms! ðŸ¤–"
            
            await store_conversation_memory(request.user_id, request.query, result, {'action': 'agent_swarm'})
            
            return {
                'message': response,
                'swarm_result': result,
                'action': 'agent_swarm',
                'response_time': f"{time.time() - start_time:.2f}s",
                'timestamp': datetime.now().isoformat()
            }
        
        # General research and conversation
        else:
            # Conduct deep web research
            research_results = await conduct_deep_web_research(request.query, 3)
            
            # Prepare comprehensive response
            context_prompt = f"""
            Query: {request.query}
            
            Web Research Results: {json.dumps(research_results, indent=2)}
            
            Previous Context: {json.dumps(memories, indent=2)}
            
            Infrastructure Status: {json.dumps(sophia_state.infrastructure_status, indent=2)}
            
            Repository Index: Available repositories: {list(sophia_state.repository_index.keys())}
            
            As SOPHIA V4, the CEO's ultimate autonomous partner, provide a comprehensive response that:
            1. Directly answers the query with specific insights
            2. Incorporates relevant web research findings
            3. References previous conversation context when relevant
            4. Suggests actionable next steps
            5. Maintains the neon cowboy personality
            
            Be specific, actionable, and demonstrate deep understanding of the ecosystem.
            """
            
            ai_response = await call_openrouter_model('primary', [
                {'role': 'user', 'content': context_prompt}
            ], max_tokens=2500)
            
            response_content = ai_response.get('content', 'Sorry partner, hit a snag there! ðŸ¤ ')
            
            # Ensure neon cowboy style
            if not response_content.startswith('ðŸ¤ '):
                response_content = f"ðŸ¤  {response_content}"
            
            if 'fucking' not in response_content.lower() and 'damn' not in response_content.lower():
                response_content += "\n\nThat's the real fucking deal! ðŸ¤ "
            
            await store_conversation_memory(request.user_id, request.query, {
                'ai_response': ai_response,
                'research_results': research_results
            }, {'action': 'general_conversation'})
            
            return {
                'message': response_content,
                'research_results': research_results,
                'memories_used': len(memories),
                'ai_model': ai_response.get('model'),
                'action': 'general_conversation',
                'response_time': f"{time.time() - start_time:.2f}s",
                'timestamp': datetime.now().isoformat()
            }
    
    except Exception as e:
        logger.error(f"Autonomous chat error: {e}")
        sentry_sdk.capture_exception(e)
        return {
            'message': f"ðŸ¤  Yo, partner! Hit a snag with '{request.query}': {str(e)}. But I'm still locked and loaded! ðŸ¤ ",
            'error': str(e),
            'response_time': f"{time.time() - start_time:.2f}s",
            'timestamp': datetime.now().isoformat()
        }

# WebSocket endpoint for real-time updates
@app.websocket("/ws/{user_id}")
async def websocket_endpoint(websocket: WebSocket, user_id: str):
    await websocket.accept()
    sophia_state.active_connections[user_id] = websocket
    
    try:
        # Send initial status
        await websocket.send_json({
            'type': 'connection_established',
            'user_id': user_id,
            'infrastructure_status': sophia_state.infrastructure_status,
            'repositories': list(sophia_state.repository_index.keys()),
            'timestamp': datetime.now().isoformat()
        })
        
        while True:
            # Keep connection alive
            await websocket.receive_text()
            
    except WebSocketDisconnect:
        if user_id in sophia_state.active_connections:
            del sophia_state.active_connections[user_id]

# Infrastructure endpoints
@app.post("/api/v1/infrastructure")
async def infrastructure_control(request: InfrastructureRequest):
    """Direct infrastructure control endpoint"""
    if request.target == 'flyio':
        result = await control_flyio_infrastructure(request.action, request.parameters or {})
    elif request.target == 'lambda':
        result = await control_lambda_infrastructure(request.action, request.parameters or {})
    else:
        result = {'success': False, 'error': f'Unknown target: {request.target}'}
    
    return result

# Repository endpoints
@app.post("/api/v1/repository")
async def repository_control(request: RepositoryRequest):
    """Direct repository control endpoint"""
    result = await execute_github_operation(request.action, request.repository, request.parameters or {})
    return result

# Agent swarm endpoints
@app.post("/api/v1/agent-swarm")
async def agent_swarm_control(request: AgentSwarmRequest):
    """Direct agent swarm control endpoint"""
    result = await create_agent_swarm(request.task, request.agents, request.parameters)
    return result

@app.get("/api/v1/agent-swarm/{swarm_id}")
async def get_agent_swarm_status(swarm_id: str):
    """Get agent swarm status"""
    swarm = sophia_state.agent_swarms.get(swarm_id)
    if not swarm:
        raise HTTPException(status_code=404, detail="Swarm not found")
    return swarm

# Health and status endpoints
@app.get("/api/v1/health")
async def health():
    """Comprehensive health check"""
    health_status = await monitor_infrastructure_health()
    
    return {
        "status": "healthy" if all(
            isinstance(status, dict) and status.get('healthy', True) 
            for status in health_status.values() 
            if isinstance(status, dict)
        ) else "degraded",
        "version": "4.0.0-ULTIMATE-ECOSYSTEM",
        "timestamp": datetime.now().isoformat(),
        "mode": "ULTIMATE_AUTONOMOUS_ECOSYSTEM",
        "personality": "neon_cowboy_ceo_partner",
        "capabilities": [
            "complete_infrastructure_control",
            "full_github_operations",
            "deep_web_research",
            "dynamic_natural_language",
            "auto_repository_indexing",
            "ai_agent_swarms",
            "zero_downtime_updates",
            "real_time_monitoring"
        ],
        "models": OPENROUTER_MODELS,
        "infrastructure": health_status,
        "repositories": list(sophia_state.repository_index.keys()),
        "active_swarms": len(sophia_state.agent_swarms),
        "active_connections": len(sophia_state.active_connections),
        "last_repo_sync": sophia_state.last_repo_sync.isoformat() if sophia_state.last_repo_sync else None,
        "response_time": "0.01s"
    }

@app.get("/api/v1/status")
async def status():
    """Detailed system status"""
    return {
        "infrastructure": sophia_state.infrastructure_status,
        "repositories": sophia_state.repository_index,
        "agent_swarms": sophia_state.agent_swarms,
        "active_connections": len(sophia_state.active_connections),
        "last_repo_sync": sophia_state.last_repo_sync.isoformat() if sophia_state.last_repo_sync else None,
        "timestamp": datetime.now().isoformat()
    }

# Background tasks
async def background_monitoring():
    """Background monitoring and maintenance"""
    while True:
        try:
            # Monitor infrastructure health
            await monitor_infrastructure_health()
            
            # Update repository index every 30 minutes
            if (not sophia_state.last_repo_sync or 
                datetime.now() - sophia_state.last_repo_sync > timedelta(minutes=30)):
                await index_repository_structure()
            
            # Clean up old agent swarms
            current_time = datetime.now()
            for swarm_id, swarm in list(sophia_state.agent_swarms.items()):
                created_at = datetime.fromisoformat(swarm['created_at'])
                if current_time - created_at > timedelta(hours=24):
                    del sophia_state.agent_swarms[swarm_id]
            
            await asyncio.sleep(300)  # Check every 5 minutes
            
        except Exception as e:
            logger.error(f"Background monitoring error: {e}")
            sentry_sdk.capture_exception(e)
            await asyncio.sleep(60)  # Wait 1 minute on error

# Start background monitoring
@app.on_event("startup")
async def startup_event():
    """Initialize SOPHIA ecosystem on startup"""
    logger.info("ðŸ¤  SOPHIA V4 Ultimate Ecosystem starting up...")
    
    # Start background monitoring
    asyncio.create_task(background_monitoring())
    
    # Initial repository indexing
    asyncio.create_task(index_repository_structure())
    
    # Initial infrastructure health check
    asyncio.create_task(monitor_infrastructure_health())
    
    logger.info("ðŸš€ SOPHIA V4 Ultimate Ecosystem ready to dominate!")

# Serve static files
app.mount("/v4", StaticFiles(directory="apps/frontend/v4", html=True), name="frontend")
app.mount("/", StaticFiles(directory="apps/frontend/v4", html=True), name="root")

if __name__ == "__main__":
    import uvicorn
    port = int(os.getenv("PORT", 8080))
    uvicorn.run(app, host="0.0.0.0", port=port)

