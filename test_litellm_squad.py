#!/usr/bin/env python3
"""
Test harness for LiteLLM Squad Router
Tests model routing, cost tracking, and squad orchestration
"""

import os
import sys
import json
import asyncio
from pathlib import Path

# Add project to path
sys.path.append(str(Path(__file__).parent))

# Mock environment if needed
os.environ['LITELLM_API_KEY'] = '09e30f5d9e3d57d5f3ae98435bda387b84d252d0c58cc10017706cb2d9b2d990'
os.environ['REDIS_HOST'] = 'localhost'
os.environ['REDIS_PORT'] = '6379'

async def test_litellm_router():
    """Test the LiteLLM router directly"""
    print("\nüß™ Testing LiteLLM Squad Router...")
    
    try:
        from app.services.litellm_squad_router import (
            IntelligentSquadRouter,
            TaskContext,
            SquadRole,
            TaskComplexity
        )
        
        # Initialize router
        router = IntelligentSquadRouter()
        print("‚úÖ Router initialized")
        
        # Test 1: Simple task (should route to economy model)
        print("\nüìù Test 1: Simple Documentation Task")
        context = TaskContext(
            squad_role=SquadRole.DOCUMENTER,
            max_cost=0.5
        )
        
        messages = [
            {"role": "user", "content": "Write a comment for this function"}
        ]
        
        # Mock response since we're testing routing logic
        print(f"  - Task complexity: {router.task_analyzer.analyze_complexity.__name__}")
        print(f"  - Assigned role: {context.squad_role.value}")
        print(f"  - Max cost: ${context.max_cost}")
        print(f"  - Expected model: gemini-assistant (economy tier)")
        
        # Test 2: Complex task (should route to premium model)
        print("\nüèóÔ∏è Test 2: Architecture Task")
        context = TaskContext(
            squad_role=SquadRole.ARCHITECT,
            files=["app/core/engine.py", "app/api/router.py"],
            mcp_servers=["filesystem", "git"]
        )
        
        messages = [
            {"role": "user", "content": "Design a microservices architecture for the authentication system"}
        ]
        
        print(f"  - Task complexity: COMPLEX")
        print(f"  - Assigned role: {context.squad_role.value}")
        print(f"  - MCP servers: {context.mcp_servers}")
        print(f"  - Expected model: claude-architect (premium tier)")
        
        # Test 3: Cost tracking
        print("\nüí∞ Test 3: Cost Tracking")
        cost_report = await router.cost_tracker.get_usage_report()
        print(f"  - Daily total: ${cost_report['daily_total']:.2f}")
        print(f"  - Daily limit: ${cost_report['daily_limit']:.2f}")
        print(f"  - Budget remaining: ${cost_report['daily_limit'] - cost_report['daily_total']:.2f}")
        
        return True
        
    except Exception as e:
        print(f"‚ùå Error: {e}")
        import traceback
        traceback.print_exc()
        return False

async def test_claude_squad_orchestrator():
    """Test the Claude Squad Orchestrator"""
    print("\nü§ñ Testing Claude Squad Orchestrator...")
    
    try:
        from sophia_squad.claude_squad_orchestrator import (
            ClaudeSquadOrchestrator,
            SquadTask,
            TaskType,
            AgentStatus
        )
        
        # Initialize orchestrator
        orchestrator = ClaudeSquadOrchestrator()
        print("‚úÖ Orchestrator initialized")
        
        # Check agents
        print("\nüë• Squad Agents:")
        for agent_id, agent in orchestrator.agents.items():
            print(f"  - {agent.name}: {agent.role.value} (Model: {agent.model_preference})")
        
        # Test task planning
        print("\nüìã Test Task Planning:")
        test_request = "Build a user authentication system with JWT tokens"
        
        # Mock task breakdown
        task_types = [
            TaskType.ARCHITECTURE,
            TaskType.IMPLEMENTATION,
            TaskType.TESTING,
            TaskType.REVIEW,
            TaskType.DOCUMENTATION
        ]
        
        print(f"  Request: {test_request}")
        print(f"  Expected task breakdown:")
        for task_type in task_types:
            print(f"    - {task_type.value}")
        
        return True
        
    except Exception as e:
        print(f"‚ùå Error: {e}")
        import traceback
        traceback.print_exc()
        return False

async def test_mcp_integration():
    """Test MCP server integration"""
    print("\nüîå Testing MCP Integration...")
    
    import httpx
    
    async with httpx.AsyncClient() as client:
        # Test Memory Server
        try:
            response = await client.get("http://localhost:8081/health")
            if response.status_code == 200:
                print("‚úÖ Memory Server: Connected")
            else:
                print("‚ùå Memory Server: Not responding")
        except:
            print("‚ùå Memory Server: Not running")
        
        # Test Filesystem Server
        try:
            response = await client.get("http://localhost:8082/health")
            if response.status_code == 200:
                print("‚úÖ Filesystem Server: Connected")
            else:
                print("‚ùå Filesystem Server: Not responding")
        except:
            print("‚ùå Filesystem Server: Not running")
        
        # Test Git Server
        try:
            response = await client.get("http://localhost:8084/health")
            if response.status_code == 200:
                print("‚úÖ Git Server: Connected")
            else:
                print("‚ùå Git Server: Not responding")
        except:
            print("‚ùå Git Server: Not running")
    
    return True

async def main():
    """Run all tests"""
    print("=" * 60)
    print("SOPHIA INTEL AI - SQUAD SYSTEM TEST SUITE")
    print("=" * 60)
    
    # Check Redis
    print("\nüî¥ Checking Redis...")
    import redis
    try:
        r = redis.Redis(host='localhost', port=6379)
        r.ping()
        print("‚úÖ Redis: Connected")
    except:
        print("‚ùå Redis: Not running (required for cost tracking)")
    
    # Run tests
    results = []
    
    # Test MCP
    results.append(await test_mcp_integration())
    
    # Test LiteLLM Router
    results.append(await test_litellm_router())
    
    # Test Claude Squad
    results.append(await test_claude_squad_orchestrator())
    
    # Summary
    print("\n" + "=" * 60)
    print("TEST SUMMARY")
    print("=" * 60)
    
    passed = sum(1 for r in results if r)
    total = len(results)
    
    print(f"Tests Passed: {passed}/{total}")
    
    if passed == total:
        print("‚úÖ All tests passed!")
    else:
        print("‚ö†Ô∏è Some tests failed. Check the output above.")
    
    return passed == total

if __name__ == "__main__":
    success = asyncio.run(main())
    sys.exit(0 if success else 1)